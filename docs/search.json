[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MY DREAMS",
    "section": "",
    "text": "Everyone has a dream…or at least most people have one. These are some of the dreams I have:\n\n\nBuying a house\nBuying a car\nFinding a great job\nTraveling to many countries\n\n\nIn order to achieve these dreams I have taken the following actions:\n\n\nStart a MS in Statistics\nSave and invest\nRead every day\nLearn a new language\nLearn Python\nWork on communication skills\n\n\nSometimes, I need a little motivation. So I look at pictures such as this one from Porche’s web site:\\\n![](https://images-porsche.imgix.net/-/media/18BDB8E5546C4BA4A12F88891D661876_76C4D93742CD42E8BEBE222C39477D82_911-carrera-4-gts-front?w=750&q=85&auto=format)"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "This is a small project in which I explore data from the National Transit Database with the objective of finding facts and insights from the data. Instructions and further details provided by my professor Mr. Michael Weylandt, Ph.D can be found here. We will be using data such as farebox revenues, total number of trips, total number of vehicle miles traveled, and total revenues and expenses by source.\n\n\nTo start with the project, I used the following code to download the data. Here we can see some tables being created with specific columns, then these new tables are joined.\n\nlibrary(tidyverse)\nlibrary(DT)\nsetwd(\"C:/Users/MT/Desktop/R Coding/STA9750-2024-FALL\")\n\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\n\n\n\nAfter downloading the data, the first task is to create a synthetic name. The column UZA Name was renamed as metro_area:\n\nUSAGE &lt;- USAGE |&gt; \n  rename(metro_area = `UZA Name`)\n\n\n\n\nMode column was recoded for easier understanding of data. Details obtained from here\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"\n))\n\nNow we can see a table with cleaner data:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nUSAGE &lt;- USAGE |&gt; \n  select(-`3 Mode`)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month = as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\n\n\n\nQ1. What transit agency had the most total VRM in our data set? The transit agency with the most total VRM (Vehicle Revenue Miles) is MTA New York City Transit with around 69 billion in total VRM.\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarise(UPT_Total = sum(UPT)) |&gt;\n  arrange(desc(UPT_Total)) |&gt;\n  head(1) |&gt;\n  ungroup()\n\n# A tibble: 1 × 2\n  Agency                      UPT_Total\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 69101730780\n\n\nQ2. What transit mode had the most total VRM in our data set? The transit mode with the most total VRM in our data set is bus with a total UPT (Unlinked Passenger Trips) of around 49 billion\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarise(VRM_Total = sum(VRM)) |&gt;\n  arrange(desc(VRM_Total)) |&gt;\n  head(1) |&gt;\n  ungroup()\n\n# A tibble: 1 × 2\n  Mode    VRM_Total\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n# The data shows bus transportation had the most vehicle revenue miles \"VRM\"\n# which is \"The miles that vehicles are scheduled to or actually travel while in revenue service\"\n# https://www.transit.dot.gov/ntd/national-transit-database-ntd-glossary#V\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024? A total of 186,478,364 trips were taken on the NYC subway\n\ncolnames(USAGE)\n\n[1] \"NTD ID\"     \"Agency\"     \"metro_area\" \"Mode\"       \"month\"     \n[6] \"UPT\"        \"VRM\"       \n\nnyc_filter_q3 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2024-05-01\") |&gt;\n  summarise(May_2024_Trips = sum(UPT, na.rm = TRUE))\nprint(nyc_filter_q3)\n\n# A tibble: 1 × 1\n  May_2024_Trips\n           &lt;dbl&gt;\n1      186478364\n\n\nQ4. What mode of transport had the longest average trip in May 2024? Note: This question can’t be answered with the current data\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020? NYC subway ridership fell by approximately 91 during the dates indicated. This could be atributed to the Corona Virus pandemic of 2020.\n\nnyc_filter_q5_part1 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarise(nyc_sub_2019 = sum(UPT, na.rm = TRUE))\nnyc_filter_q5_part2 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarise(nyc_sub_2020 = sum(UPT, na.rm = TRUE))\nnyc_filter_q5_part3=((nyc_filter_q5_part2-nyc_filter_q5_part1)/(nyc_filter_q5_part1))*100\nprint(nyc_filter_q5_part3)\n\n  nyc_sub_2020\n1    -91.39649\n\n\n\n\n\nQ4.1 Which Agency has the most VRM per UPT? I chose this question because I believe this shows how efficiently an agency is using its resources. A lower Vehicle Revenue Miles to Unlinked Passenger Trips ratio would show that an Agency is able to maximize the usage of its vehicles in relation to the number of passanger trips. In this case the most efficient based on the previously mentioned definition would be New York City Department of Transportation with a ratio of .00942. However, this needs deeper analysis to determine its implications and flaws\n\nvrm_per_upt &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarise(UPT_Total = sum(UPT), VRM_Total = sum(VRM), VRM_per_UPT_ratio = (VRM_Total/UPT_Total)) |&gt;\n  ungroup() |&gt;\n  arrange(VRM_per_UPT_ratio) |&gt;\n  head(5)\n\nprint(vrm_per_upt)\n\n# A tibble: 5 × 4\n  Agency                                   UPT_Total VRM_Total VRM_per_UPT_ratio\n  &lt;chr&gt;                                        &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 New York City Department of Transportat… 440538576   4150573           0.00942\n2 Plaquemines Port, Harbor and Terminal D…   1397902     37290           0.0267 \n3 Washington State Ferries                 502394973  20230767           0.0403 \n4 Kansas City, City of Missouri             14171285   1074129           0.0758 \n5 City of Portland                          56396362   4562513           0.0809 \n\n\nQ4.2 What is the most used mode per metro area? The most used mode per metro area is the bus\n\nmost_used_mode_per_metro_area &lt;- USAGE |&gt;\n  group_by(metro_area, Mode) |&gt;\n  summarise(total_upt = sum(UPT, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(metro_area) |&gt;\n  slice(which.max(total_upt)) |&gt;\n  select(metro_area, Mode, total_upt)\nprint(most_used_mode_per_metro_area)\n\n# A tibble: 376 × 3\n# Groups:   metro_area [376]\n   metro_area                   Mode  total_upt\n   &lt;chr&gt;                        &lt;chr&gt;     &lt;dbl&gt;\n 1 Abilene, TX                  Bus     4811813\n 2 Akron, OH                    Bus   133397482\n 3 Albany, GA                   Bus    15828372\n 4 Albany--Schenectady, NY      Bus   312961295\n 5 Albuquerque, NM              Bus   200432456\n 6 Alexandria, LA               Bus     6168051\n 7 Allentown--Bethlehem, PA--NJ Bus   101405562\n 8 Altoona, PA                  Bus    13053250\n 9 Amarillo, TX                 Bus     4408831\n10 Ames, IA                     Bus   109854287\n# ℹ 366 more rows\n\n\nQ4.3 What are the top metro areas by mode? New York Metro area is the top area by what I consider to be the most important modes of transportation: Heavy Rail, Bus, and Commuter Rail\n\ntop_metro_areas_by_mode &lt;- USAGE |&gt;\n  group_by(Mode, metro_area) |&gt;\n  summarise(total_upt = sum(UPT, na.rm = TRUE)) |&gt;\n  group_by(Mode) |&gt;\n  slice(which.max(total_upt)) |&gt;\n  arrange(desc(total_upt))\n\nprint(top_metro_areas_by_mode)\n\n# A tibble: 18 × 3\n# Groups:   Mode [18]\n   Mode                        metro_area                              total_upt\n   &lt;chr&gt;                       &lt;chr&gt;                                       &lt;dbl&gt;\n 1 Heavy Rail                  New York--Jersey City--Newark, NY--NJ 53450544938\n 2 Bus                         New York--Jersey City--Newark, NY--NJ 23394737094\n 3 Commuter Rail               New York--Jersey City--Newark, NY--NJ  5430746046\n 4 Light Rail                  Boston, MA--NH                         1372586910\n 5 Trolleybus                  San Francisco--Oakland, CA             1345222391\n 6 Ferryboat                   New York--Jersey City--Newark, NY--NJ   556020462\n 7 Publico                     San Juan, PR                            516329565\n 8 Bus Rapid Transit           New York--Jersey City--Newark, NY--NJ   315432467\n 9 Commuter Bus                New York--Jersey City--Newark, NY--NJ   286645521\n10 Streetcar Rail              Philadelphia, PA--NJ--DE--MD            263034317\n11 Demand Response             New York--Jersey City--Newark, NY--NJ   146574222\n12 Cable Car                   San Francisco--Oakland, CA              138759762\n13 Monorail Automated Guideway Miami--Fort Lauderdale, FL              101162479\n14 Vanpool                     Seattle--Tacoma, WA                      87386804\n15 Hybrid Rail                 New York--Jersey City--Newark, NY--NJ    30022908\n16 Aerial Tramway              Portland, OR--WA                         16258014\n17 Inclined Plane              Pittsburgh, PA                           13141618\n18 Alaska Railroad             Anchorage, AK                             2938715\n\n\n\n\n\nIn this part of the project, I created a new table from USAGE that has annual total (sum) UPT and VRM for 2022\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == \"2022\") |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarise(UPT_Total = sum(UPT, na.rm = TRUE), VRM_Total = sum(VRM, na.rm = TRUE)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, FINANCIALS, \n  join_by(`NTD ID`, \"Mode\")) |&gt;\n  drop_na()\n\ncolnames(USAGE_2022_ANNUAL)\n\n[1] \"NTD ID\"     \"Agency\"     \"metro_area\" \"Mode\"       \"UPT_Total\" \n[6] \"VRM_Total\" \n\nsample_n(USAGE_2022_ANNUAL, 1141) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n\n\nNote I did not restrict data to major transit systems because I was interested in seeing the results despite the size of the transit system\nQ1. Which transit system (agency and mode) had the most UPT in 2022? The transit system with the most UPT in 2022 was the MTA New York City Transit\n\nUSAGE_2022_ANNUAL |&gt;\n  group_by(Agency, Mode) |&gt;\n  arrange(desc(UPT_Total)) |&gt;\n  head(5) |&gt;\n  ungroup()\n\n# A tibble: 5 × 6\n  `NTD ID` Agency                           metro_area Mode  UPT_Total VRM_Total\n     &lt;int&gt; &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1    20008 MTA New York City Transit        New York-… Heav…    1.79e9 338199451\n2    20008 MTA New York City Transit        New York-… Bus      4.59e8  82638609\n3    90154 Los Angeles County Metropolitan… Los Angel… Bus      1.94e8 124306796\n4    50066 Chicago Transit Authority        Chicago, … Bus      1.40e8  44199272\n5    20080 New Jersey Transit Corporation   New York-… Bus      1.13e8 161834490\n\n\nQ2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nThe Agency Transit Authority of Central Kentucky with the mode of Vanpool had the highest farebox recovery. This indicates that this agency was able to obtain more revenue relative to its expenses in comparison to other Agencies. It its interesting to see that Vanpool occupies the second and third place in this query. This might indicate that this is Mode of transportation is financially efficient\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(Expenses, na.rm = TRUE))) |&gt;\n  ungroup() |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\n\nprint(highest_farebox_recovery)\n\n# A tibble: 5 × 3\n  Agency                                                        Mode      ratio\n  &lt;chr&gt;                                                         &lt;chr&gt;     &lt;dbl&gt;\n1 Transit Authority of Central Kentucky                         Vanpool    2.38\n2 County of Miami-Dade                                          Vanpool    1.67\n3 Yuma County Intergovernmental Public Transportation Authority Vanpool    1.47\n4 Port Imperial Ferry Corporation                               Ferryboat  1.43\n5 Hyannis Harbor Tours, Inc.                                    Ferryboat  1.41\n\n\nQ3. Which transit system (agency and mode) has the lowest expenses per UPT?\nNorth Carolina State University’s Bus transportation had the lowest expenses per UPT. This seems to be a transit system focusing on transportation for a university, so it might be that the low expenses derive from the relative short range of the service provided\n\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_UPT)\n\n# A tibble: 5 × 3\n  Agency                          Mode           ratio\n  &lt;chr&gt;                           &lt;chr&gt;          &lt;dbl&gt;\n1 North Carolina State University Bus             1.18\n2 Anaheim Transportation Network  Bus             1.28\n3 Valley Metro Rail, Inc.         Streetcar Rail  1.49\n4 University of Iowa              Bus             1.54\n5 Chatham Area Transit Authority  Ferryboat       1.60\n\n\nQ4. Which transit system (agency and mode) has the highest total fares per UPT?\nThe Altoona Metro Transit Demand Response transportation had the highest total fares per UPT\n\nhighest_total_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_total_fares_per_UPT) \n\n# A tibble: 5 × 3\n  Agency                                        Mode            ratio\n  &lt;chr&gt;                                         &lt;chr&gt;           &lt;dbl&gt;\n1 Altoona Metro Transit                         Demand Response 660. \n2 Alaska Railroad Corporation                   Alaska Railroad 153. \n3 Bay State LLC                                 Ferryboat        65.0\n4 Central Pennsylvania Transportation Authority Demand Response  50.2\n5 Hampton Jitney, Inc.                          Commuter Bus     41.3\n\n\nQ5. Which transit system (agency and mode) has the lowest expenses per VRM?\nNew Mexico Department of Transportation Vanpool mode had the lowest expense per VRM. That same mode is in the next 4 spots of our results. This indicates that Vanpool could be transportation mode that does not require or demands a lot of expenses.\n\nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_VRM)\n\n# A tibble: 5 × 3\n  Agency                                           Mode    ratio\n  &lt;chr&gt;                                            &lt;chr&gt;   &lt;dbl&gt;\n1 New Mexico Department of Transportation          Vanpool 0.337\n2 VIA Metropolitan Transit                         Vanpool 0.370\n3 County of Miami-Dade                             Vanpool 0.386\n4 County of Volusia                                Vanpool 0.393\n5 Corpus Christi Regional Transportation Authority Vanpool 0.431\n\n\nQ6. Which transit system (agency and mode) has the highest total fares per VRM?\nChicago Water Taxi (Wendella) Ferryboat transportation had the highest total fares per VRM. This might reflect the fact that expenses and mainteinance for this mode of transportation is higher\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_fares_per_VRM)\n\n# A tibble: 5 × 3\n  Agency                                             Mode            ratio\n  &lt;chr&gt;                                              &lt;chr&gt;           &lt;dbl&gt;\n1 Chicago Water Taxi (Wendella)                      Ferryboat        237.\n2 Altoona Metro Transit                              Demand Response  229.\n3 Jacksonville Transportation Authority              Ferryboat        158.\n4 Chattanooga Area Regional Transportation Authority Inclined Plane   149.\n5 Hyannis Harbor Tours, Inc.                         Ferryboat        138.\n\n\n\n\n\nFarebox recovery rate for the Transit Authority of Central Kentucky, and more specifically the Vanpool mode seems to be great. There should be a deeper investigation into why this is, and how this transportation mode can be escalated to serve more people, or how Vanpool transportation mode’s characteristics can be leveraged for other transportation modes to achieve such recovery rates"
  },
  {
    "objectID": "mp01.html#preparing-the-data",
    "href": "mp01.html#preparing-the-data",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "To start with the project, I used the following code to download the data. Here we can see some tables being created with specific columns, then these new tables are joined.\n\nlibrary(tidyverse)\nlibrary(DT)\nsetwd(\"C:/Users/MT/Desktop/R Coding/STA9750-2024-FALL\")\n\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()"
  },
  {
    "objectID": "mp01.html#creating-syntatic-names",
    "href": "mp01.html#creating-syntatic-names",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "After downloading the data, the first task is to create a synthetic name. The column UZA Name was renamed as metro_area:\n{r echo=TRUE, warning=FALSE, message=FALSE}\nUSAGE &lt;- USAGE |&gt; \n  rename(metro_area = `UZA Name`)"
  },
  {
    "objectID": "mp01.html#task-1-creating-syntatic-names",
    "href": "mp01.html#task-1-creating-syntatic-names",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "After downloading the data, the first task is to create a synthetic name. The column UZA Name was renamed as metro_area:\n\nUSAGE &lt;- USAGE |&gt; \n  rename(metro_area = `UZA Name`)"
  },
  {
    "objectID": "mp01.html#task-2-recoding-the-mode-column",
    "href": "mp01.html#task-2-recoding-the-mode-column",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Mode column was recoded for easier understanding of data. Details obtained from here\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"\n))\n\nNow we can see a table with cleaner data:\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nUSAGE &lt;- USAGE |&gt; \n  select(-`3 Mode`)\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month = as.character(month)) |&gt; \n  DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-3-answering-instructor-specified-questions-with-dplyr",
    "href": "mp01.html#task-3-answering-instructor-specified-questions-with-dplyr",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Q1. What transit agency had the most total VRM in our data set? The transit agency with the most total VRM (Vehicle Revenue Miles) is MTA New York City Transit with around 69 billion in total VRM.\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarise(UPT_Total = sum(UPT)) |&gt;\n  arrange(desc(UPT_Total)) |&gt;\n  head(1) |&gt;\n  ungroup()\n\n# A tibble: 1 × 2\n  Agency                      UPT_Total\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 69101730780\n\n\nQ2. What transit mode had the most total VRM in our data set? The transit mode with the most total VRM in our data set is bus with a total UPT (Unlinked Passenger Trips) of around 49 billion\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarise(VRM_Total = sum(VRM)) |&gt;\n  arrange(desc(VRM_Total)) |&gt;\n  head(1) |&gt;\n  ungroup()\n\n# A tibble: 1 × 2\n  Mode    VRM_Total\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n# The data shows bus transportation had the most vehicle revenue miles \"VRM\"\n# which is \"The miles that vehicles are scheduled to or actually travel while in revenue service\"\n# https://www.transit.dot.gov/ntd/national-transit-database-ntd-glossary#V\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024? A total of 186,478,364 trips were taken on the NYC subway\n\ncolnames(USAGE)\n\n[1] \"NTD ID\"     \"Agency\"     \"metro_area\" \"Mode\"       \"month\"     \n[6] \"UPT\"        \"VRM\"       \n\nnyc_filter_q3 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2024-05-01\") |&gt;\n  summarise(May_2024_Trips = sum(UPT, na.rm = TRUE))\nprint(nyc_filter_q3)\n\n# A tibble: 1 × 1\n  May_2024_Trips\n           &lt;dbl&gt;\n1      186478364\n\n\nQ4. What mode of transport had the longest average trip in May 2024? Note: This question can’t be answered with the current data\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020? NYC subway ridership fell by approximately 91 during the dates indicated. This could be atributed to the Corona Virus pandemic of 2020.\n\nnyc_filter_q5_part1 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarise(nyc_sub_2019 = sum(UPT, na.rm = TRUE))\nnyc_filter_q5_part2 &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarise(nyc_sub_2020 = sum(UPT, na.rm = TRUE))\nnyc_filter_q5_part3=((nyc_filter_q5_part2-nyc_filter_q5_part1)/(nyc_filter_q5_part1))*100\nprint(nyc_filter_q5_part3)\n\n  nyc_sub_2020\n1    -91.39649"
  },
  {
    "objectID": "mp01.html#task-4-explore-and-analyze-find-three-more-interesting-transit-facts-in-this-data-other-than-those-above.",
    "href": "mp01.html#task-4-explore-and-analyze-find-three-more-interesting-transit-facts-in-this-data-other-than-those-above.",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Q4.1 Which Agency has the most VRM per UPT? I chose this question because I believe this shows how efficiently an agency is using its resources. A lower Vehicle Revenue Miles to Unlinked Passenger Trips ratio would show that an Agency is able to maximize the usage of its vehicles in relation to the number of passanger trips. In this case the most efficient based on the previously mentioned definition would be New York City Department of Transportation with a ratio of .00942. However, this needs deeper analysis to determine its implications and flaws\n\nvrm_per_upt &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarise(UPT_Total = sum(UPT), VRM_Total = sum(VRM), VRM_per_UPT_ratio = (VRM_Total/UPT_Total)) |&gt;\n  ungroup() |&gt;\n  arrange(VRM_per_UPT_ratio) |&gt;\n  head(5)\n\nprint(vrm_per_upt)\n\n# A tibble: 5 × 4\n  Agency                                   UPT_Total VRM_Total VRM_per_UPT_ratio\n  &lt;chr&gt;                                        &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 New York City Department of Transportat… 440538576   4150573           0.00942\n2 Plaquemines Port, Harbor and Terminal D…   1397902     37290           0.0267 \n3 Washington State Ferries                 502394973  20230767           0.0403 \n4 Kansas City, City of Missouri             14171285   1074129           0.0758 \n5 City of Portland                          56396362   4562513           0.0809 \n\n\nQ4.2 What is the most used mode per metro area? The most used mode per metro area is the bus\n\nmost_used_mode_per_metro_area &lt;- USAGE |&gt;\n  group_by(metro_area, Mode) |&gt;\n  summarise(total_upt = sum(UPT, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(metro_area) |&gt;\n  slice(which.max(total_upt)) |&gt;\n  select(metro_area, Mode, total_upt)\nprint(most_used_mode_per_metro_area)\n\n# A tibble: 376 × 3\n# Groups:   metro_area [376]\n   metro_area                   Mode  total_upt\n   &lt;chr&gt;                        &lt;chr&gt;     &lt;dbl&gt;\n 1 Abilene, TX                  Bus     4811813\n 2 Akron, OH                    Bus   133397482\n 3 Albany, GA                   Bus    15828372\n 4 Albany--Schenectady, NY      Bus   312961295\n 5 Albuquerque, NM              Bus   200432456\n 6 Alexandria, LA               Bus     6168051\n 7 Allentown--Bethlehem, PA--NJ Bus   101405562\n 8 Altoona, PA                  Bus    13053250\n 9 Amarillo, TX                 Bus     4408831\n10 Ames, IA                     Bus   109854287\n# ℹ 366 more rows\n\n\nQ4.3 What are the top metro areas by mode? New York Metro area is the top area by what I consider to be the most important modes of transportation: Heavy Rail, Bus, and Commuter Rail\n\ntop_metro_areas_by_mode &lt;- USAGE |&gt;\n  group_by(Mode, metro_area) |&gt;\n  summarise(total_upt = sum(UPT, na.rm = TRUE)) |&gt;\n  group_by(Mode) |&gt;\n  slice(which.max(total_upt)) |&gt;\n  arrange(desc(total_upt))\n\nprint(top_metro_areas_by_mode)\n\n# A tibble: 18 × 3\n# Groups:   Mode [18]\n   Mode                        metro_area                              total_upt\n   &lt;chr&gt;                       &lt;chr&gt;                                       &lt;dbl&gt;\n 1 Heavy Rail                  New York--Jersey City--Newark, NY--NJ 53450544938\n 2 Bus                         New York--Jersey City--Newark, NY--NJ 23394737094\n 3 Commuter Rail               New York--Jersey City--Newark, NY--NJ  5430746046\n 4 Light Rail                  Boston, MA--NH                         1372586910\n 5 Trolleybus                  San Francisco--Oakland, CA             1345222391\n 6 Ferryboat                   New York--Jersey City--Newark, NY--NJ   556020462\n 7 Publico                     San Juan, PR                            516329565\n 8 Bus Rapid Transit           New York--Jersey City--Newark, NY--NJ   315432467\n 9 Commuter Bus                New York--Jersey City--Newark, NY--NJ   286645521\n10 Streetcar Rail              Philadelphia, PA--NJ--DE--MD            263034317\n11 Demand Response             New York--Jersey City--Newark, NY--NJ   146574222\n12 Cable Car                   San Francisco--Oakland, CA              138759762\n13 Monorail Automated Guideway Miami--Fort Lauderdale, FL              101162479\n14 Vanpool                     Seattle--Tacoma, WA                      87386804\n15 Hybrid Rail                 New York--Jersey City--Newark, NY--NJ    30022908\n16 Aerial Tramway              Portland, OR--WA                         16258014\n17 Inclined Plane              Pittsburgh, PA                           13141618\n18 Alaska Railroad             Anchorage, AK                             2938715"
  },
  {
    "objectID": "mp01.html#task-5-table-summarization",
    "href": "mp01.html#task-5-table-summarization",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "In this part of the project, I created a new table from USAGE that has annual total (sum) UPT and VRM for 2022\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == \"2022\") |&gt;\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  summarise(UPT_Total = sum(UPT, na.rm = TRUE), VRM_Total = sum(VRM, na.rm = TRUE)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, FINANCIALS, \n  join_by(`NTD ID`, \"Mode\")) |&gt;\n  drop_na()\n\ncolnames(USAGE_2022_ANNUAL)\n\n[1] \"NTD ID\"     \"Agency\"     \"metro_area\" \"Mode\"       \"UPT_Total\" \n[6] \"VRM_Total\" \n\nsample_n(USAGE_2022_ANNUAL, 1141) |&gt; \n    DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "href": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Note I did not restrict data to major transit systems because I was interested in seeing the results despite the size of the transit system\nQ1. Which transit system (agency and mode) had the most UPT in 2022? The transit system with the most UPT in 2022 was the MTA New York City Transit\n{r echo=TRUE, warning=FALSE, message=FALSE}\nUSAGE_2022_ANNUAL |&gt;\n  group_by(Agency, Mode) |&gt;\n  arrange(desc(UPT_Total)) |&gt;\n  head(5) |&gt;\n  ungroup()\n\nQ2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nThe Agency Transit Authority of Central Kentucky with the mode of Vanpool had the highest farebox recovery. This indicates that this agency was able to obtain more revenue relative to its expenses in comparison to other Agencies. It its interesting to see that Vanpool occupies the second and third place in this query. This might indicate that this is Mode of transportation is financially efficient\n{r echo=TRUE, warning=FALSE, message=FALSE}\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(Expenses, na.rm = TRUE))) |&gt;\n  ungroup() |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\n\nprint(highest_farebox_recovery)\n\nQ3. Which transit system (agency and mode) has the lowest expenses per UPT?\nNorth Carolina State University’s Bus transportation had the lowest expenses per UPT. This seems to be a transit system focusing on transportation for a university, so it might be that the low expenses derive from the relative short range of the service provided\n{r echo=TRUE, warning=FALSE, message=FALSE}\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_UPT)\n\nQ4. Which transit system (agency and mode) has the highest total fares per UPT?\nThe Altoona Metro Transit Demand Response transportation had the highest total fares per UPT\n{r echo=TRUE, warning=FALSE, message=FALSE}\nhighest_total_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_total_fares_per_UPT) \n\nQ5. Which transit system (agency and mode) has the lowest expenses per VRM?\nNew Mexico Department of Transportation Vanpool mode had the lowest expense per VRM. That same mode is in the next 4 spots of our results. This indicates that Vanpool could be transportation mode that does not require or demands a lot of expenses.\n{r echo=TRUE, warning=FALSE, message=FALSE}\nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_VRM)\n\nQ6. Which transit system (agency and mode) has the highest total fares per VRM?\nChicago Water Taxi (Wendella) Ferryboat transportation had the highest total fares per VRM. This might reflect the fact that expenses and mainteinance for this mode of transportation is higher\n{r echo=TRUE, warning=FALSE, message=FALSE}\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_fares_per_VRM)\n\n##Conclusion"
  },
  {
    "objectID": "mp01.html#task-6-farebox-recovery-among-all-systems",
    "href": "mp01.html#task-6-farebox-recovery-among-all-systems",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Note I did not restrict data to major transit systems because I was interested in seeing the results despite the size of the transit system\nQ1. Which transit system (agency and mode) had the most UPT in 2022? The transit system with the most UPT in 2022 was the MTA New York City Transit\n\nUSAGE_2022_ANNUAL |&gt;\n  group_by(Agency, Mode) |&gt;\n  arrange(desc(UPT_Total)) |&gt;\n  head(5) |&gt;\n  ungroup()\n\n# A tibble: 5 × 6\n  `NTD ID` Agency                           metro_area Mode  UPT_Total VRM_Total\n     &lt;int&gt; &lt;chr&gt;                            &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1    20008 MTA New York City Transit        New York-… Heav…    1.79e9 338199451\n2    20008 MTA New York City Transit        New York-… Bus      4.59e8  82638609\n3    90154 Los Angeles County Metropolitan… Los Angel… Bus      1.94e8 124306796\n4    50066 Chicago Transit Authority        Chicago, … Bus      1.40e8  44199272\n5    20080 New Jersey Transit Corporation   New York-… Bus      1.13e8 161834490\n\n\nQ2. Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nThe Agency Transit Authority of Central Kentucky with the mode of Vanpool had the highest farebox recovery. This indicates that this agency was able to obtain more revenue relative to its expenses in comparison to other Agencies. It its interesting to see that Vanpool occupies the second and third place in this query. This might indicate that this is Mode of transportation is financially efficient\n\nhighest_farebox_recovery &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(Expenses, na.rm = TRUE))) |&gt;\n  ungroup() |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\n\nprint(highest_farebox_recovery)\n\n# A tibble: 5 × 3\n  Agency                                                        Mode      ratio\n  &lt;chr&gt;                                                         &lt;chr&gt;     &lt;dbl&gt;\n1 Transit Authority of Central Kentucky                         Vanpool    2.38\n2 County of Miami-Dade                                          Vanpool    1.67\n3 Yuma County Intergovernmental Public Transportation Authority Vanpool    1.47\n4 Port Imperial Ferry Corporation                               Ferryboat  1.43\n5 Hyannis Harbor Tours, Inc.                                    Ferryboat  1.41\n\n\nQ3. Which transit system (agency and mode) has the lowest expenses per UPT?\nNorth Carolina State University’s Bus transportation had the lowest expenses per UPT. This seems to be a transit system focusing on transportation for a university, so it might be that the low expenses derive from the relative short range of the service provided\n\nlowest_expenses_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_UPT)\n\n# A tibble: 5 × 3\n  Agency                          Mode           ratio\n  &lt;chr&gt;                           &lt;chr&gt;          &lt;dbl&gt;\n1 North Carolina State University Bus             1.18\n2 Anaheim Transportation Network  Bus             1.28\n3 Valley Metro Rail, Inc.         Streetcar Rail  1.49\n4 University of Iowa              Bus             1.54\n5 Chatham Area Transit Authority  Ferryboat       1.60\n\n\nQ4. Which transit system (agency and mode) has the highest total fares per UPT?\nThe Altoona Metro Transit Demand Response transportation had the highest total fares per UPT\n\nhighest_total_fares_per_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(UPT_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_total_fares_per_UPT) \n\n# A tibble: 5 × 3\n  Agency                                        Mode            ratio\n  &lt;chr&gt;                                         &lt;chr&gt;           &lt;dbl&gt;\n1 Altoona Metro Transit                         Demand Response 660. \n2 Alaska Railroad Corporation                   Alaska Railroad 153. \n3 Bay State LLC                                 Ferryboat        65.0\n4 Central Pennsylvania Transportation Authority Demand Response  50.2\n5 Hampton Jitney, Inc.                          Commuter Bus     41.3\n\n\nQ5. Which transit system (agency and mode) has the lowest expenses per VRM?\nNew Mexico Department of Transportation Vanpool mode had the lowest expense per VRM. That same mode is in the next 4 spots of our results. This indicates that Vanpool could be transportation mode that does not require or demands a lot of expenses.\n\nlowest_expenses_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(Expenses, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(ratio) |&gt;\n  head(5)\nprint(lowest_expenses_per_VRM)\n\n# A tibble: 5 × 3\n  Agency                                           Mode    ratio\n  &lt;chr&gt;                                            &lt;chr&gt;   &lt;dbl&gt;\n1 New Mexico Department of Transportation          Vanpool 0.337\n2 VIA Metropolitan Transit                         Vanpool 0.370\n3 County of Miami-Dade                             Vanpool 0.386\n4 County of Volusia                                Vanpool 0.393\n5 Corpus Christi Regional Transportation Authority Vanpool 0.431\n\n\nQ6. Which transit system (agency and mode) has the highest total fares per VRM?\nChicago Water Taxi (Wendella) Ferryboat transportation had the highest total fares per VRM. This might reflect the fact that expenses and mainteinance for this mode of transportation is higher\n\nhighest_fares_per_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarise(ratio = (sum(`Total Fares`, na.rm = TRUE)/sum(VRM_Total, na.rm = TRUE)),.groups = \"drop\") |&gt;\n  arrange(desc(ratio)) |&gt;\n  head(5)\nprint(highest_fares_per_VRM)\n\n# A tibble: 5 × 3\n  Agency                                             Mode            ratio\n  &lt;chr&gt;                                              &lt;chr&gt;           &lt;dbl&gt;\n1 Chicago Water Taxi (Wendella)                      Ferryboat        237.\n2 Altoona Metro Transit                              Demand Response  229.\n3 Jacksonville Transportation Authority              Ferryboat        158.\n4 Chattanooga Area Regional Transportation Authority Inclined Plane   149.\n5 Hyannis Harbor Tours, Inc.                         Ferryboat        138."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "Farebox recovery rate for the Transit Authority of Central Kentucky, and more specifically the Vanpool mode seems to be great. There should be a deeper investigation into why this is, and how this transportation mode can be escalated to serve more people, or how Vanpool transportation mode’s characteristics can be leveraged for other transportation modes to achieve such recovery rates"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "In this project, I will put myself in the shoes of a Hollywood development executive. I will be analyzing data from the Internet Movie Database (IMDb) with the objective of proposing a new Hollywood project based on the insights provided by the data. To achieve the objective, I will first analyze historical data containing components such as genres, directors, and ratings to find trends and insights.\n\n\nWe will start by downloading the data from the Internet Movie Database (IMDb). The following code will automatically download and load these files into R:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\n\n\nGiven this is a large amount of data, we are going to need to start down-selecting to facilitate analysis performance. For our NAME_BASICS table, we’ll restrict our attention to people with at least two “known for” credits.1\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nAs we can see in the following histogram, IMDb contains a long tail of not well known movies:\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nTo improve computer efficiency, any title with less than 100 ratings will be removed. As seen in the following table, this action drops around 75% of the data set:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2952034 \n\n\nBy applying the following code, we significantly reduce the size of the data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nThe same filtering will be applied to the other TITLE_* tables. In the following case, the semi_join is used. The semi_join returns only values which have a match, but doesn’t add columns.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\nAt this point, the data has been filtered down significantly. Now, the analysis process can be started.\n\n\n\nWe will start examining our data more closely.\n\nprint(\"NAME_BASICS TABLE\")\n\n[1] \"NAME_BASICS TABLE\"\n\nglimpse(NAME_BASICS, width = 100, max.extra.cols = Inf)\n\nRows: 3,188,805\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", \"nm0000005\", \"nm0000…\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", \"John Belushi\", \"Ing…\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"1899\", \"1924\", \"1925\", …\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957\", \"2004\", \"1984\", \"…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,archive_footage\", \"a…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt0037382,tt0075213,tt011705…\n\nprint(\"TITLE_BASICS TABLE\")\n\n[1] \"TITLE_BASICS TABLE\"\n\nglimpse(TITLE_BASICS, width = 100, max.extra.cols = Inf)\n\nRows: 374,145\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"mo…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Poor Pierrot\", \"Un bon bock\", \"Bla…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot\", \"Un bon bock\", \"B…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ startYear      &lt;chr&gt; \"1894\", \"1892\", \"1892\", \"1892\", \"1893\", \"1894\", \"1894\", \"1894\", \"1894\", \"18…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\"…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Comedy,Romance\", \"Animat…\n\nprint(\"TITLE_CREW TABLE\")\n\n[1] \"TITLE_CREW TABLE\"\n\nglimpse(TITLE_CREW, width = 100, max.extra.cols = Inf)\n\nRows: 373,840\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006\", \"t…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm0005690\", \"nm0005690\", \"n…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0085156\", \"\\\\N\", \"\\\\N…\n\nprint(\"TITLE_EPISODES TABLE\")\n\n[1] \"TITLE_EPISODES TABLE\"\n\nglimpse(TITLE_EPISODES, width = 100, max.extra.cols = Inf)\n\nRows: 3,022,865\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0049025\", \"tt0049473\"…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0047720\", \"tt0046593\"…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"1\", \"8\", \"1\", \"10\", …\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\", \"3\", \"2\", \"10\", \"1…\n\nprint(\"TITLE_PRINCIPALS TABLE\")\n\n[1] \"TITLE_PRINCIPALS TABLE\"\n\nglimpse(TITLE_PRINCIPALS, width = 100, max.extra.cols = Inf)\n\nRows: 6,619,063\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000002\", \"tt0000002\", \"…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721526\", \"nm1335271\", \"…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"director\", \"composer\", \"dir…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"prod…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\…\n\nprint(\"TITLE_RATINGS TABLE\")\n\n[1] \"TITLE_RATINGS TABLE\"\n\nglimpse(TITLE_RATINGS, width = 100, max.extra.cols = Inf)\n\nRows: 374,145\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006\"…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7.4, 5.7, 7.1, 6.1, 5…\n$ numVotes      &lt;dbl&gt; 2096, 283, 2103, 183, 2839, 197, 889, 2243, 215, 7728, 400, 13138, 2012, 597…"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "In this project, I will put myself in the shoes of a Hollywood development executive. I will be analyzing data from the Internet Movie Database (IMDb) with the objective of proposing a new Hollywood project based on the insights provided by the data. To achieve the objective, I will first analyze historical data containing components such as genres, directors, and ratings to find trends and insights.\n\n\nWe will start by downloading the data from the Internet Movie Database (IMDb). The following code will automatically download and load these files into R:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\n\n\nGiven this is a large amount of data, we are going to need to start down-selecting to facilitate analysis performance. For our NAME_BASICS table, we’ll restrict our attention to people with at least two “known for” credits.1\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nAs we can see in the following histogram, IMDb contains a long tail of not well known movies:\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nTo improve computer efficiency, any title with less than 100 ratings will be removed. As seen in the following table, this action drops around 75% of the data set:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2952034 \n\n\nBy applying the following code, we significantly reduce the size of the data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nThe same filtering will be applied to the other TITLE_* tables. In the following case, the semi_join is used. The semi_join returns only values which have a match, but doesn’t add columns.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\nAt this point, the data has been filtered down significantly. Now, the analysis process can be started.\n\n\n\nWe will start examining our data more closely.\n\nprint(\"NAME_BASICS TABLE\")\n\n[1] \"NAME_BASICS TABLE\"\n\nglimpse(NAME_BASICS, width = 100, max.extra.cols = Inf)\n\nRows: 3,188,805\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", \"nm0000005\", \"nm0000…\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", \"John Belushi\", \"Ing…\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"1899\", \"1924\", \"1925\", …\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957\", \"2004\", \"1984\", \"…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,archive_footage\", \"a…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt0037382,tt0075213,tt011705…\n\nprint(\"TITLE_BASICS TABLE\")\n\n[1] \"TITLE_BASICS TABLE\"\n\nglimpse(TITLE_BASICS, width = 100, max.extra.cols = Inf)\n\nRows: 374,145\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"mo…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Poor Pierrot\", \"Un bon bock\", \"Bla…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot\", \"Un bon bock\", \"B…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ startYear      &lt;chr&gt; \"1894\", \"1892\", \"1892\", \"1892\", \"1893\", \"1894\", \"1894\", \"1894\", \"1894\", \"18…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\"…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Comedy,Romance\", \"Animat…\n\nprint(\"TITLE_CREW TABLE\")\n\n[1] \"TITLE_CREW TABLE\"\n\nglimpse(TITLE_CREW, width = 100, max.extra.cols = Inf)\n\nRows: 373,840\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006\", \"t…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm0005690\", \"nm0005690\", \"n…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0085156\", \"\\\\N\", \"\\\\N…\n\nprint(\"TITLE_EPISODES TABLE\")\n\n[1] \"TITLE_EPISODES TABLE\"\n\nglimpse(TITLE_EPISODES, width = 100, max.extra.cols = Inf)\n\nRows: 3,022,865\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0049025\", \"tt0049473\"…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0047720\", \"tt0046593\"…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"1\", \"8\", \"1\", \"10\", …\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\", \"3\", \"2\", \"10\", \"1…\n\nprint(\"TITLE_PRINCIPALS TABLE\")\n\n[1] \"TITLE_PRINCIPALS TABLE\"\n\nglimpse(TITLE_PRINCIPALS, width = 100, max.extra.cols = Inf)\n\nRows: 6,619,063\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000002\", \"tt0000002\", \"…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721526\", \"nm1335271\", \"…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"director\", \"composer\", \"dir…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"prod…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\…\n\nprint(\"TITLE_RATINGS TABLE\")\n\n[1] \"TITLE_RATINGS TABLE\"\n\nglimpse(TITLE_RATINGS, width = 100, max.extra.cols = Inf)\n\nRows: 374,145\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0000005\", \"tt0000006\"…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7.4, 5.7, 7.1, 6.1, 5…\n$ numVotes      &lt;dbl&gt; 2096, 283, 2103, 183, 2839, 197, 889, 2243, 215, 7728, 400, 13138, 2012, 597…"
  },
  {
    "objectID": "mp02.html#putting-it-together",
    "href": "mp02.html#putting-it-together",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Putting It Together",
    "text": "Putting It Together"
  },
  {
    "objectID": "mp02.html#general-remarks",
    "href": "mp02.html#general-remarks",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "General Remarks",
    "text": "General Remarks\nAs you approach this project, recall there are no right or wrong answers. You are exploring data looking for exciting and actionable findings. You have several key decisions to make and you can support them with data, but the decisions are ultimately yours. This project is an exercise both in the “nuts-and-bolts” of analyzing a large data set and in using data to inform and refine what is ultimately still a “gut feeling” qualitative business decision.\n\nCredits to Michael Weylandt who provided a great part of the code for the data gathering and cleaning process."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s not entirely transparent who IMDb decides what projects an actor or director is “known for”. Still, it’s a reasonable filter that leaves us with more than enough to work with for this project.↩︎\nIn order to see that a movie has not been recently remade, it is sufficient to confirm that no movie has been made with the same name in the past 25 years.↩︎"
  },
  {
    "objectID": "mp02.html#task-2-instructor-provided-questions",
    "href": "mp02.html#task-2-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 2: Instructor-Provided Questions",
    "text": "Task 2: Instructor-Provided Questions\n\nTask 2 Question 1\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\n\nTo answer these questions, we will be grouping our TITLE_BASICS table by title type to count how many titles of each type there are, and then we will be filtering by the specific title type we are asked to provide. Please see code and output below:\n\nTITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarise(count = n()) |&gt;\n  filter(titleType == \"movie\" | titleType == \"tvSeries\" | titleType == \"tvEpisode\")\n\n# A tibble: 3 × 2\n  titleType  count\n  &lt;chr&gt;      &lt;int&gt;\n1 movie     132220\n2 tvEpisode 156725\n3 tvSeries   29986\n\n\nFrom the table obtained, we can see that there are many more movies and TV Episodes than TV Series.\n\n\nTask 2 Question 2\n\nWho is the oldest living person in our data set?\n\nGiven the data include Hollywood personalities with birth year but no death year, I will filter by those born after 1904.\n\ncurrent_year &lt;- year(Sys.Date())\n\noldest_living_person &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt; 1904, is.na(deathYear)) |&gt;\n  arrange(birthYear) |&gt;\n  mutate(age = current_year - birthYear)\n\noldest_living_person |&gt;\n  select(primaryName, birthYear, age) |&gt;\n  distinct(primaryName, .keep_all = TRUE) |&gt;\n  group_by(primaryName) |&gt;\n  head(5)\n\n# A tibble: 5 × 3\n# Groups:   primaryName [5]\n  primaryName       birthYear   age\n  &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n1 Julio Abadía           1905   119\n2 Michael Arco           1905   119\n3 Aleksandr Astafev      1905   119\n4 A. Barr-Smith          1905   119\n5 Max A. Bienek          1905   119\n\n\nBased on the results, and the data that we have, we cannot definitely determine who is the oldest person alive in our data set. In the same way there are individuals in our data born in 1625 with no death year, the output received could include deceased individuals with no death year available but who are deceased. If for grading purposes I must chose one, I would say Julio Abadía is the oldest living person in our data set. Julio Abadía was born in Bogota Colombia, and is known for La Rosa de Francia.\n\n\nTask 2 Question 3\n\nThere is one TV Episode in this data set with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\n\nThe TV Episode with a perfect 10/10 rating and at least 200,000 votes is Ozymandias. To find this, I fully joined three tables and filtered by the parameters provided. To find which series Ozymandias belongs to, I created a column that matches the tconst of the parent to the tconst in the TITLE_BASICS table and returns the title. Thus, we can see that Ozymandias belongs to the series Breaking Bad.\nAt this point, we take note of the genres, which could give us some insight into which genra we could do a new production on.\n\nTITLE_BASICS_RATINGS_EPISODES &lt;- full_join(TITLE_BASICS, TITLE_RATINGS, join_by(tconst)) |&gt; full_join(TITLE_EPISODES, join_by(tconst))\n\nTITLE_BASICS_RATINGS_EPISODES |&gt; \n  filter(titleType == \"tvEpisode\", averageRating == 10, numVotes &gt;= 200000) |&gt;\n  mutate(parentTitle = TITLE_BASICS$originalTitle[match(parentTconst, TITLE_BASICS$tconst)]) |&gt;\n  select(-c(tconst, isAdult, endYear, primaryTitle)) |&gt;\n  head() |&gt;\n  glimpse(width = 100, max.extra.cols = Inf)\n\nRows: 1\nColumns: 11\n$ titleType      &lt;chr&gt; \"tvEpisode\"\n$ originalTitle  &lt;chr&gt; \"Ozymandias\"\n$ startYear      &lt;dbl&gt; 2013\n$ runtimeMinutes &lt;dbl&gt; 47\n$ genres         &lt;chr&gt; \"Crime,Drama,Thriller\"\n$ averageRating  &lt;dbl&gt; 10\n$ numVotes       &lt;dbl&gt; 229845\n$ parentTconst   &lt;chr&gt; \"tt0903747\"\n$ seasonNumber   &lt;dbl&gt; 5\n$ episodeNumber  &lt;dbl&gt; 14\n$ parentTitle    &lt;chr&gt; \"Breaking Bad\"\n\n\n\n\nTask 2 Question 4\n\nWhat four projects is the actor Mark Hamill most known for?\n\nThe actor Mark Hamill is most known for the four Star Wars projects listed below.\n\n#Expand the knownForTitles values into rows with a single title. I am doing this in order to match each title with its name\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  separate_longer_delim(col = knownForTitles, delim = \",\")\n\n\n#Now I can join the table above with another table containing the name of the titles. I am doing a left join to ensure I keep all data from the NAME_BASICS table regardless of there being a match or not\nNAME_TITLE_BASICS &lt;- left_join(NAME_BASICS, TITLE_BASICS, join_by(knownForTitles == tconst)) |&gt;\n  rename(knownForTitle = knownForTitles)\n\n#Now, I use the newly created table to answer the question (I asked the system to give me 20 results to ensure I am not missing any titles)\nNAME_TITLE_BASICS |&gt; \n  filter(primaryName == \"Mark Hamill\") |&gt;\n  select(primaryName, titleType, originalTitle, knownForTitle, genres, startYear) |&gt;\n  head(20) |&gt;\n  glimpse(width = 100, max.extra.cols = Inf)\n\nRows: 4\nColumns: 6\n$ primaryName   &lt;chr&gt; \"Mark Hamill\", \"Mark Hamill\", \"Mark Hamill\", \"Mark Hamill\"\n$ titleType     &lt;chr&gt; \"movie\", \"movie\", \"movie\", \"movie\"\n$ originalTitle &lt;chr&gt; \"Star Wars\", \"Star Wars: Episode VIII - The Last Jedi\", \"Star Wars: Episode …\n$ knownForTitle &lt;chr&gt; \"tt0076759\", \"tt2527336\", \"tt0080684\", \"tt0086190\"\n$ genres        &lt;chr&gt; \"Action,Adventure,Fantasy\", \"Action,Adventure,Fantasy\", \"Action,Adventure,Fa…\n$ startYear     &lt;dbl&gt; 1977, 2017, 1980, 1983\n\n\nIt is interesting to see that this actor is know for four related projects (Star Wars) and that the start year of these projects span across multiple time periods.\n\n\nTask 2 Question 5\n\nWhat TV series, with more than 12 episodes, has the highest average rating?\n\nThe TV series with more than 12 episodes and the highest average rating is 20 Dakika.\n\nEpisodes_Basics_Ratings &lt;- full_join(TITLE_EPISODES, TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n  full_join(TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  mutate(parentTitle = TITLE_BASICS$originalTitle[match(parentTconst, TITLE_BASICS$tconst)])\n\nEpisodes_Basics_Ratings |&gt;\n  group_by(parentTconst) |&gt;\n  summarise(\n    Average_Rating = mean(averageRating, na.rm = TRUE),\n    Total_Episodes = n(),\n    Average_Votes = mean(numVotes)\n  ) |&gt;\n  filter(Total_Episodes &gt; 12, Average_Votes &gt;= 300) |&gt;\n  arrange(desc(Average_Rating)) |&gt;\n  mutate(parentTitle = TITLE_BASICS$originalTitle[match(parentTconst, TITLE_BASICS$tconst)]) |&gt;\n  ungroup()\n\n# A tibble: 925 × 5\n   parentTconst Average_Rating Total_Episodes Average_Votes parentTitle         \n   &lt;chr&gt;                 &lt;dbl&gt;          &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;               \n 1 tt2544148              9.65             25         1115. 20 Dakika           \n 2 tt6162588              9.58             32          912. Cesur Ve Güzel      \n 3 tt6384676              9.54             20         4187. Bu Sehir Arkandan G…\n 4 tt4312318              9.35             26         4105. Seref Meselesi      \n 5 tt3052000              9.34             54         4066. Günesi Beklerken    \n 6 tt22727518             9.3              16         7449. Bambaska Biri       \n 7 tt14899624             9.27             16         1445. Zeytin Agaci        \n 8 tt13661118             9.26             14         4409. Not Me              \n 9 tt9471962              9.18             19         4619. Halka               \n10 tt27460313             9.14             40          427. Yu Gu Yao           \n# ℹ 915 more rows\n\n\nLets examine the this TV series more closely.\n\nTITLE_BASICS_RATINGS_EPISODES |&gt;\n  filter(parentTconst == \"tt2544148\") |&gt;\n  select(-c(endYear,tconst, originalTitle)) |&gt;\n  glimpse(width = 100, max.extra.cols = Inf)\n\nRows: 25\nColumns: 11\n$ titleType      &lt;chr&gt; \"tvEpisode\", \"tvEpisode\", \"tvEpisode\", \"tvEpisode\", \"tvEpisode\", \"tvEpisode…\n$ primaryTitle   &lt;chr&gt; \"Demirbahce\", \"Rumelihan\", \"Cellat Cesmesi\", \"Kanli Kuyu\", \"Cihangir Camii\"…\n$ isAdult        &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ startYear      &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ runtimeMinutes &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ genres         &lt;chr&gt; \"Action,Drama,Romance\", \"Action,Drama,Romance\", \"Action,Drama,Romance\", \"Ac…\n$ averageRating  &lt;dbl&gt; 9.5, 9.6, 9.6, 9.6, 9.5, 9.4, 9.5, 9.7, 9.7, 9.7, 9.5, 9.6, 9.6, 9.6, 9.8, …\n$ numVotes       &lt;dbl&gt; 1567, 1538, 1273, 1241, 1241, 1233, 1304, 1312, 1310, 1308, 1045, 1049, 105…\n$ parentTconst   &lt;chr&gt; \"tt2544148\", \"tt2544148\", \"tt2544148\", \"tt2544148\", \"tt2544148\", \"tt2544148…\n$ seasonNumber   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ episodeNumber  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 12, 16, 18, 19, 15, 17, 20, 21, …\n\n\nFrom the above, we can see that the genres are Action, Drama, and Romance. Furthermore, the start year was 2013, which is not that old. This could indicate that contemporary audience is receptive to this kind of genre combination.\n\n\nTask 2 Question 6\n\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\nLet us start by looking at the average ratings per season and the average number of votes.\n\nEpisodes_Basics_Ratings |&gt;\n  filter(titleType == \"tvEpisode\", parentTitle == \"Happy Days\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(Average_Rating = mean(averageRating, na.rm = TRUE),\n            Total_Episodes = n(),\n            Average_Votes = mean(numVotes)) |&gt;\n    ungroup()\n\n# A tibble: 11 × 4\n   seasonNumber Average_Rating Total_Episodes Average_Votes\n          &lt;dbl&gt;          &lt;dbl&gt;          &lt;int&gt;         &lt;dbl&gt;\n 1            1           7.58             16          229.\n 2            2           7.69             23          187.\n 3            3           7.7              24          146.\n 4            4           7.43             25          125.\n 5            5           7                24          118.\n 6            6           7.02             16          118.\n 7            7           6.33              3          103.\n 8            8           5.3               1          108 \n 9            9           6.4               1          100 \n10           10           6.7               1          114 \n11           11           7.33              3          113 \n\n\nFrom the table, we can clearly see that the average number of votes declined as seasons passed. To better visualize the relationship between the ratings and seasons we will create a linear model\n\n#Name data to be inputted into the linear model\nHappy_Days_Data &lt;- Episodes_Basics_Ratings |&gt;\n  filter(titleType == \"tvEpisode\", parentTitle == \"Happy Days\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(Average_Rating = mean(averageRating, na.rm = TRUE),\n            Total_Episodes = n(),\n            Average_Votes = mean(numVotes)) |&gt;\n    ungroup()\n\n#Create linear model\nlinear_model &lt;- lm(Average_Rating ~ seasonNumber, data = Happy_Days_Data)\n\n# Create graph\nif(!require(\"ggpmisc\")) install.packages(\"ggpmisc\")\n\nLoading required package: ggpmisc\n\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(ggpmisc)\nggplot(Happy_Days_Data, aes(x = seasonNumber, y = Average_Rating)) +\n  geom_point() +\n  stat_poly_line(se=FALSE, color=\"black\") +\n  labs(\n    title = \"Happy Days: Average Rating by Season\",\n    x = \"Season Number\",\n    y = \"Average Rating\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nFrom the graph above, we see that episodes from later seasons of Happy Days have lower average ratings than the early seasons. However, as we saw initially, lower ratings come from a smaller number of rating votes. This could reflect lost interest due to lower quality of the series, or from many in the audience replacing this show for another one."
  },
  {
    "objectID": "mp02.html#task-3-instructor-provided-questions",
    "href": "mp02.html#task-3-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 3: Instructor-Provided Questions",
    "text": "Task 3: Instructor-Provided Questions\nIn this section we will be creating a success metric. It will be derived solely on the data we have due to data licensing constraints.\n\nTask 3\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness. Implement your success metric using a mutate operator to add a new column to the TITLE_RATINGS table. Validate your success metric as follows (see task 3 questions and responses below).\nIn order to create this measure, first I would like to see a how the number of votes for each title in our TITLE_RATINGS table are distributed. Remember we dropped titles with less than 100 ratings.\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nBased on this data. I would like to create a measure that leverages the ratings provided by individuals, but weights the rating by the number of people who voted. Therefore, I will weight titles by the quantile of the number of votes because this would give the rating a sense of popular awareness.\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n    100     165     332     971 2952034 \n\n\nBased on the table above, we create the weighted rating\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(quantile_weight = cut(numVotes,\n  breaks = quantile(numVotes,\n  probs = c(0, 0.25, 0.5, 0.75, 1)),\n  labels = c(0.25, 0.5, 0.75, 1)),\n  weighted_rating = averageRating * as.numeric(as.character(quantile_weight)))\n\n\n\nTask 3 Question 1\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nYes, the movies were indeed box office successes.\n\nNEW_TITLE_RATINGS &lt;- full_join(TITLE_RATINGS, TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n  full_join(TITLE_EPISODES, join_by(tconst == tconst)) |&gt;\n  mutate(parentTitle = TITLE_BASICS$originalTitle[match(parentTconst, TITLE_BASICS$tconst)])\n\nNEW_TITLE_RATINGS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  arrange(desc(weighted_rating)) |&gt;\n  select(-parentTitle, -episodeNumber, -seasonNumber, -parentTconst, -isAdult, -tconst, -quantile_weight, -averageRating, -endYear, -originalTitle, -runtimeMinutes) |&gt;\n  head(10)\n\n   numVotes weighted_rating titleType        primaryTitle startYear\n1      1025            10.0     movie              Kaveri      2024\n2       994             9.9     movie        Kaalapatthar      2024\n3       980             9.8     movie      Uruttu Factory      2024\n4      1092             9.6     movie   Adharma Kadhaigal      2024\n5      1012             9.6     movie             Aaragan      2024\n6      1038             9.5     movie     Laughing Buddha      2024\n7      1062             9.5     movie    Operation Raavan      2024\n8      4113             9.5     movie          Bahumukham      2024\n9      2072             9.5     movie Janaka Aithe Ganaka      2024\n10     2086             9.5     movie      Ramnagar Bunny      2024\n           genres\n1          Action\n2    Action,Drama\n3     Documentary\n4             \\\\N\n5           Drama\n6          Comedy\n7           Crime\n8  Drama,Thriller\n9           Drama\n10         Comedy\n\n\n\n\nTask 3 Question 2\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nThe following shows titles for which the maximum weight was applied and yet the rating was poor.\n\nNEW_TITLE_RATINGS |&gt;\n  filter(titleType == \"movie\", quantile_weight == 1) |&gt;\n  arrange(weighted_rating) |&gt;\n  select(-parentTitle, -episodeNumber, -seasonNumber, -parentTconst, -isAdult, -tconst, -averageRating, -endYear, -originalTitle, -numVotes, -startYear, -runtimeMinutes, -genres) |&gt;\n  head(5)\n\n  quantile_weight weighted_rating titleType\n1               1               1     movie\n2               1               1     movie\n3               1               1     movie\n4               1               1     movie\n5               1               1     movie\n                          primaryTitle\n1                           321 Action\n2 2025 - The World enslaved by a Virus\n3   Holnap történt - A nagy bulvárfilm\n4                                 Reis\n5       Cumali Ceber: Allah Seni Alsin\n\n\n\n\nTask 3 Question 3\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nI chose Leonardo DiCaprio. The success metric aligns with the many projects with high scores in which he participated.\n\n#Find Leonardo DiCaprio's nconst\nNAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt; select(nconst,primaryName) |&gt; head(1)\n\n     nconst       primaryName\n1 nm0000138 Leonardo DiCaprio\n\n\n\n#Show Leonardo DiCaprio's movies and weighted rating using descending order based on the rating\n\ndicaprio_movies &lt;- \n  TITLE_PRINCIPALS |&gt;\n  filter(nconst == \"nm0000138\") |&gt;\n  select(nconst, tconst) |&gt;\n  inner_join(NEW_TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  select(weighted_rating, primaryTitle) |&gt;\n  distinct(primaryTitle, .keep_all = TRUE) |&gt;\n  arrange(desc(weighted_rating)) |&gt;\n  head(10)\n\ndicaprio_movies\n\n   weighted_rating                          primaryTitle\n1              9.5                             Episode X\n2              8.8                             Inception\n3              8.8 Inception: Jump Right Into the Action\n4              8.5                          The Departed\n5              8.5                      Django Unchained\n6              8.2               The Wolf of Wall Street\n7              8.2                        Shutter Island\n8              8.2                        The First Born\n9              8.1                   Catch Me If You Can\n10             8.0                         Blood Diamond\n\n\n\n\nTask 3 Question 4\nPerform at least one other form of ‘spot check’ validation.\nThe TV series Mad Men is considered by many (such as me and my friends) to be a masterpiece. The weighted rating correctly reflects this taking into consideration public awareness of the series.\n\nNEW_TITLE_RATINGS |&gt;\n  filter(primaryTitle == \"Mad Men\") |&gt;\n  arrange(desc(weighted_rating)) |&gt;\n  select(-quantile_weight,-isAdult,-originalTitle,-parentTconst,-seasonNumber,-episodeNumber,-parentTitle, -tconst, -numVotes, -startYear, -endYear, -runtimeMinutes, -averageRating) |&gt;\n  head(1)\n\n  weighted_rating titleType primaryTitle genres\n1             8.7  tvSeries      Mad Men  Drama\n\n\n\n\nTask 3 Question 5\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value \\(v\\) such that movies above \\(v\\) are all “solid” or better.\nYou will use your success metric and threshold to complete the rest of this Mini-Project. You may, if you wish, restrict your attention to movies for the remainder of your analysis, though a good development executive should also consider making TV series.\nI consider that the success metric created will accurately show success of a title. Success, for the purposes of this project will be defined as having a weighted rating of at least 7.5. Remember that this takes into consideration public awareness. Therefore, titles with low public awareness will not be shown in our modified table.\n\nMODIFIED_TITLE_RATINGS &lt;- NEW_TITLE_RATINGS |&gt;\n  filter(weighted_rating &gt;= 7.5)"
  },
  {
    "objectID": "mp02.html#task-1-column-type-correction-instructor-provided",
    "href": "mp02.html#task-1-column-type-correction-instructor-provided",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 1: Column Type Correction (Instructor-Provided)",
    "text": "Task 1: Column Type Correction (Instructor-Provided)\nCorrect the column types of the TITLE tables using a combination of mutate and the coercion functions as.numeric and as.logical.\n\nTask 1: Column Type Correction\nThe use of the glimpse function above allows us to examine each table. We can see that many columns appear to be read as character (string) vectors, even when they should be read as numeric, or numeric when they should be read as logical. This could occur because missing values are represented as \\\\N or other non numeric values. Since R does not know that these are NA values, it retains them as strings.\nTo fix these, we will use the mutate command, and the as.numeric or as.logical command to change column type as follows:\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate(startYear = as.numeric(startYear),\n           endYear = as.numeric(endYear),\n           runtimeMinutes = as.numeric(runtimeMinutes),\n           isAdult = as.logical(isAdult))\n\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n    mutate(seasonNumber = as.numeric(seasonNumber),\n           episodeNumber = as.numeric(episodeNumber))\n\nAnother aspect of this data that we would like to address is that it combines multiple pieces of information in a single cell separated by commas. We see this in the NAME_BASICS table, where both the primaryProfession and knownForTitles columns combine multiple values.\n\nglimpse(NAME_BASICS)\n\nRows: 3,188,805\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nWe can use the separate_longer_delim function to break these into multiple rows: for example\n\nNAME_BASICS |&gt; separate_longer_delim(knownForTitles, \",\") |&gt; slice_head(n = 10) |&gt; glimpse(width = 100, max.extra.cols = Inf)\n\nRows: 10\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000001\", \"nm0000001\", \"nm0000001\", \"nm0000002\", \"nm0000…\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Fred Astaire\", \"Fred Astaire\", \"Fred Astaire\", \"Lauren …\n$ birthYear         &lt;dbl&gt; 1899, 1899, 1899, 1899, 1924, 1924, 1924, 1924, 1934, 1934\n$ deathYear         &lt;dbl&gt; 1987, 1987, 1987, 1987, 2014, 2014, 2014, 2014, NA, NA\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actor,miscellaneous,producer\", \"actor,m…\n$ knownForTitles    &lt;chr&gt; \"tt0072308\", \"tt0050419\", \"tt0053137\", \"tt0027125\", \"tt0037382\", \"tt0075…\n\n\nTo preserve flexibility, we will not fully separate NAME_BASICS just yet, but we will need to use separate_longer_delim to answer various questions."
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decadeinstructor-provided-questions",
    "href": "mp02.html#examining-success-by-genre-and-decadeinstructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Examining Success by Genre and Decade:Instructor-Provided Questions",
    "text": "Examining Success by Genre and Decade:Instructor-Provided Questions\nNow that you have a working proxy for success, it’s time to look at trends in success over time. Answer the following questions. Your responses should include at least 2 graphics."
  },
  {
    "objectID": "mp02.html#task-4-trends-in-success-over-time-instructor-provided-questions",
    "href": "mp02.html#task-4-trends-in-success-over-time-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 4: Trends in Success Over Time (Instructor-Provided Questions)",
    "text": "Task 4: Trends in Success Over Time (Instructor-Provided Questions)\nUsing questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\nTask 4 Question 1\nWhat was the genre with the most “successes” in each decade?\nTo answer this question I will look at each genre separately and then together with its other related genres.\nLet’s look at successful individual genre per decade:\n\nMODIFIED_TITLE_RATINGS |&gt;\n  distinct(parentTconst, .keep_all = TRUE) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  mutate(\n    genres = trimws(genres),\n    decade = paste0(floor(startYear / 10) * 10, \"-\", floor(startYear / 10) * 10 + 9)\n  ) |&gt;\n  group_by(decade, genres) |&gt;\n  summarise(mean_weighted_rating = mean(weighted_rating), .groups = \"drop\") |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = mean_weighted_rating, n = 1) |&gt;\n  select(decade, genres, mean_weighted_rating) |&gt;\n  ungroup()\n\n# A tibble: 12 × 3\n   decade    genres    mean_weighted_rating\n   &lt;chr&gt;     &lt;chr&gt;                    &lt;dbl&gt;\n 1 1900-1909 Action                    8.1 \n 2 1900-1909 Adventure                 8.1 \n 3 1900-1909 Comedy                    8.1 \n 4 1950-1959 Comedy                    9.5 \n 5 1960-1969 Horror                    8.2 \n 6 1960-1969 Mystery                   8.2 \n 7 1970-1979 Comedy                    8.6 \n 8 1980-1989 War                       8.85\n 9 1990-1999 Music                     9.3 \n10 2000-2009 Family                    9.28\n11 2010-2019 News                      9.1 \n12 2020-2029 News                      9.5 \n\n\nThe current decade’s most successful genre is News with an average weighted rating of 9.5.\n\n\nTask 4 Question 2\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nDrama has consistently had the most success. Based on the graphs, the genres Mistery and Romance seem to be the ones in decline for a while. However, those have not many “successes” as compared to other genres. The genres with most successes seem to have a fairly stable variation of success.\n\nMODIFIED_TITLE_RATINGS |&gt;\n  mutate(\n    parentTconst = ifelse(titleType == \"tvSeries\", parentTconst, tconst)\n  ) |&gt;\n  distinct(parentTconst, .keep_all = TRUE) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  mutate(\n    genres = trimws(genres)\n  ) |&gt;\n  filter(startYear &gt;= 2010) |&gt;\n  group_by(startYear, genres) |&gt;\n  summarise(title_count = n(), .groups = \"drop\") |&gt;\n  ggplot(aes(x = startYear, weight = title_count)) +\n  geom_histogram(bins = 10, color = \"black\", fill = \"black\") +\n  facet_wrap(~ genres, scales = \"free_x\", shrink = TRUE, strip.position = \"bottom\") +\n  labs(title = \"Title Count by Year and Genre\", x = \"Year\", y = \"Title Count\") +\n  theme_bw() +\n  theme(\n    plot.title = element_text(size = 12, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8, family = \"sans\"),\n    strip.text = element_text(size = 10, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\nTask 4 Question 3\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nDrama has produced the most “successes” since 2010. There are many productions in that genre the same way there are many productions in Comedy. However, Comedy does not show as much success as Drama despite being the closes to the number of productions as shown below. Therefore, I conclude that Drama does create the most “successes.”\nThe following table shows total “successes” per genre given our definition:\n\nMODIFIED_TITLE_RATINGS |&gt;\n  mutate(parentTconst = ifelse(titleType == \"tvSeries\", parentTconst, tconst)) |&gt;\n  distinct(parentTconst, .keep_all = TRUE) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  mutate(genres = trimws(genres)) |&gt;\n  filter(startYear &gt;= 2010) |&gt;\n  group_by(genres) |&gt;\n  summarise(total_title_count = n()) |&gt;\n  arrange(desc(total_title_count)) |&gt;\n  ungroup()\n\n# A tibble: 27 × 2\n   genres    total_title_count\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Drama                 12799\n 2 Action                 6970\n 3 Comedy                 5557\n 4 Adventure              5435\n 5 Crime                  5081\n 6 Animation              3904\n 7 Mystery                2571\n 8 Romance                1965\n 9 Fantasy                1964\n10 Horror                 1487\n# ℹ 17 more rows\n\n\nThe following table shows total title count by genre using our initial data (not taking our success measure into consideration):\n\nTITLE_BASICS |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  group_by(genres) |&gt;\n  summarise(total_title_count = n()) |&gt;\n  arrange(desc(total_title_count)) |&gt;\n  ungroup()\n\n# A tibble: 29 × 2\n   genres      total_title_count\n   &lt;chr&gt;                   &lt;int&gt;\n 1 Drama                  177185\n 2 Comedy                 126366\n 3 Action                  72347\n 4 Crime                   60989\n 5 Adventure               57111\n 6 Animation               47568\n 7 Romance                 41456\n 8 Mystery                 33202\n 9 Thriller                27860\n10 Documentary             27580\n# ℹ 19 more rows\n\n\nThe following histograms show total successful titles by genre since 2010:\n\nMODIFIED_TITLE_RATINGS |&gt;\n  mutate(\n    parentTconst = ifelse(titleType == \"tvSeries\", parentTconst, tconst)\n  ) |&gt;\n  distinct(parentTconst, .keep_all = TRUE) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  mutate(\n    genres = trimws(genres)\n  ) |&gt;\n  filter(startYear &gt;= 2010) |&gt;\n  group_by(startYear, genres) |&gt;\n  summarise(title_count = n(), .groups = \"drop\") |&gt;\n  ggplot(aes(x = startYear, weight = title_count)) +\n  geom_histogram(bins = 10, color = \"black\", fill = \"black\", orientation = \"x\") +\n  facet_wrap(~ genres, scales = \"free_x\", shrink = TRUE, strip.position = \"bottom\") +\n  labs(title = \"Title Count by Year and Genre since 2010\", x = \"Year\", y = \"Title Count\") +\n  theme_bw() +\n  theme(\n    plot.title = element_text(size = 18, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12, family = \"sans\"),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    panel.spacing = unit(2, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\n\nTask 4 Question 4\nWhat genre has become more popular in recent years?\nBased on your findings, select a genre for your next project. Note that you may wish to avoid an “oversatured” genre; you just need to make the argument that your proposal is a good investment, not necessarily the most studio-produced focus-grouped committee-designed generic satisfying choice, so feel free to lean in to your own artistic preferences, as long as you can make an argument for them.\nAs we have seen in the graph above, and as we see in the graph below, Drama has been and is becoming more popular in recent years. For this reason, I will propose the Drama genre. Some might think this is an over saturated genre. However, based on the data and graphs we previously saw, it seems that the great amount of supply of Drama titles is driven by consumer demand.\n\nMODIFIED_TITLE_RATINGS |&gt;\n  distinct(parentTconst, .keep_all = TRUE) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n  mutate(\n    genres = trimws(genres)\n  ) |&gt;\n  filter(startYear &gt;= 2010) |&gt;\n  group_by(startYear, genres) |&gt;\n  summarise(mean_weighted_rating = mean(weighted_rating), .groups = \"drop\") |&gt;\n  ggplot(aes(x = factor(startYear), y = mean_weighted_rating)) +\n  geom_point() +\n  labs(title = \"Mean Weighted Rating by Year (Since 2010)\") +\n  theme_bw() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8, family = \"sans\"),\n    plot.title = element_text(size = 12, hjust = 0.5),\n    axis.labels = element_blank(),\n    strip.text = element_text(size = 10, face = \"bold\")\n  ) +\n  facet_wrap(~ genres, scales = \"free_y\", labeller = labeller(genres = label_wrap_gen()))"
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre-instructor-provided-questions",
    "href": "mp02.html#successful-personnel-in-the-genre-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Successful Personnel in the Genre: Instructor-Provided Questions",
    "text": "Successful Personnel in the Genre: Instructor-Provided Questions\nNow that you have selected a target genre, identify two actors and one director who will anchor your project. You want to identify key personnel who have worked in the genre before, with at least modest success, and who have at least one major success to their credit.\nAs you develop your team, you may want to consider the following possibilities:\n\nAn older established actor and an up-and-coming actor\nAn actor/director pair who have been successful together\nAn actor/director pair who are both highly successful but have never worked together\nA pair of established actors who have had success in many genres\n\nAs you select your key personnel, consider what IMDb says they are known for; this will be useful in developing your marketing materials."
  },
  {
    "objectID": "mp02.html#task-5-key-personnel-instructor-provided-questions",
    "href": "mp02.html#task-5-key-personnel-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 5: Key Personnel: Instructor-Provided Questions",
    "text": "Task 5: Key Personnel: Instructor-Provided Questions\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\nTask 5: Key Personnel\nI chose: Storm Reid as the young actress, Richard Schiff as the experienced actor, and Daniel Attias as the experienced director.\nFinding a young actor (born after 2000) with experience and sucess:\n\nNEW_TITLE_RATINGS_WITH_BIRTHYEAR &lt;- \n  TITLE_PRINCIPALS |&gt;\n  left_join(NEW_TITLE_RATINGS, join_by(tconst == tconst)) |&gt;\n  left_join(NAME_TITLE_BASICS, join_by(nconst == nconst))\n\nsuccessful_young_actor &lt;- \n  NEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(genres.x == \"Drama\", \n         !is.na(nconst), \n         !is.na(primaryName), \n         birthYear &gt; 2000) |&gt;\n  group_by(nconst, primaryName, birthYear, weighted_rating) |&gt;\n  summarise(mean_weighted_rating = mean(weighted_rating), .groups = \"drop\") |&gt;\n  arrange(desc(mean_weighted_rating))\n\nsuccessful_young_actor |&gt; head(10)\n\n# A tibble: 10 × 5\n   nconst     primaryName         birthYear weighted_rating mean_weighted_rating\n   &lt;chr&gt;      &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n 1 nm10837236 Deepali Gautam           2001             9.5                  9.5\n 2 nm4977564  Storm Reid               2003             9.5                  9.5\n 3 nm9937520  Javon 'Wanna' Walt…      2006             9.5                  9.5\n 4 nm8915757  Susanna Skaggs           2001             9.4                  9.4\n 5 nm10837236 Deepali Gautam           2001             9.3                  9.3\n 6 nm10837236 Deepali Gautam           2001             9.2                  9.2\n 7 nm10837236 Deepali Gautam           2001             9.1                  9.1\n 8 nm9937520  Javon 'Wanna' Walt…      2006             9.1                  9.1\n 9 nm14291803 Ashraful Tushar          2003             9                    9  \n10 nm4977564  Storm Reid               2003             9                    9  \n\n\nFinding an experienced actor:\n\nmost_experienced_actor &lt;- NEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(genres.x == \"Drama\", !is.na(nconst), !is.na(primaryName)) |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarise(title_count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(title_count))\n\nmost_experienced_actor |&gt; head(10)\n\n# A tibble: 10 × 3\n   nconst    primaryName         title_count\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;int&gt;\n 1 nm0771493 Richard Schiff             1140\n 2 nm0212858 Liz Dean                    848\n 3 nm0505448 Hagai Levi                  820\n 4 nm0001624 Oliver Platt                768\n 5 nm0003896 Eric Dawson                 768\n 6 nm0471740 Carol Kritzer               744\n 7 nm0580924 S. Epatha Merkerson         724\n 8 nm0815070 Aaron Sorkin                716\n 9 nm0937725 Dick Wolf                   712\n10 nm1631709 Matt Olmstead               708\n\n\nRichard Schiff’s age is 69 and he has been part of 285 titles\n\nrichard_schiff_YOB &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Richard Schiff\") |&gt;\n  pull(birthYear) |&gt;\n  unique()\n\n2024 - richard_schiff_YOB\n\n[1] 69\n\n\nFinding an experienced director:\n\nmost_experienced_director &lt;- NEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(genres.x == \"Drama\", job == \"director\", !is.na(nconst), !is.na(primaryName), !is.na(job)) |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarise(title_count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(title_count))\n\n\nmost_experienced_director |&gt;\n  head(10)\n\n# A tibble: 10 × 3\n   nconst    primaryName         title_count\n   &lt;chr&gt;     &lt;chr&gt;                     &lt;int&gt;\n 1 nm4904858 Mohammad Kart                76\n 2 nm0003733 Daniel Attias                52\n 3 nm9778554 Hakan Inan                   52\n 4 nm1246883 Mac Alejandre                40\n 5 nm0482774 Joel Lamangan                36\n 6 nm2639311 Kit Williamson               36\n 7 nm0054219 Juan Antonio Bardem          32\n 8 nm0386382 Ryûichi Hiroki               32\n 9 nm0407990 Im Kwon-taek                 32\n10 nm0606845 Patrick R. Norris            32\n\n\nDaniel Attias has been involved in 52 titles.\nThe tables above show successful individuals in the Drama genre. They are part of the data that made the cut for our definition of success. And all of them have a lot of experience. Even the young actress, is one of the most experienced out of the filtered group. As we can see in the following graph, this makes a great team which displays experience and quality. Although Storm Reid has less experience than the rest of the team, she is a young actress with a continually increasing rated titles.\n\nTHE_TEAM &lt;- NEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\nfilter(primaryName %in% c(\"Storm Reid\", \"Richard Schiff\", \"Daniel Attias\"))\n\nSUMMARY_THE_TEAM &lt;- THE_TEAM |&gt;\ngroup_by(primaryName, startYear.x) |&gt;\nsummarise(rating = mean(weighted_rating, na.rm = TRUE), .groups = \"drop\")\n\n\nggplot(SUMMARY_THE_TEAM, aes(x = factor(startYear.x), y = rating)) +\ngeom_bar(stat = \"identity\") +\nlabs(title = \"Average Ratings Across Years\",\nsubtitle = \"Storm Reid, Richard Schiff, Daniel Attias\",\nx = \"Year\",\ny = \"Average Rating\") +\ntheme_classic() +\ntheme(axis.text.x = element_blank(),\naxis.ticks.x = element_blank()) +\nfacet_wrap(~ primaryName)\n\n\n\n\n\n\n\n\nBelow, we can see what title in the Drama genre they are known for:\n\nNEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(primaryName %in% c(\"Storm Reid\", \"Richard Schiff\", \"Daniel Attias\")) |&gt;\n  left_join(TITLE_BASICS, by = c(\"tconst\" = \"tconst\")) |&gt;\n  mutate(KnownForTitleName = primaryTitle) |&gt;\n  filter(genres == \"Drama\") |&gt;\n  distinct(primaryName, .keep_all = TRUE) |&gt;\n  select(primaryName, primaryTitle.x)\n\n     primaryName                   primaryTitle.x\n1 Richard Schiff                    The West Wing\n2  Daniel Attias                     Kingdom Come\n3     Storm Reid Stand Still Like the Hummingbird"
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes-instructor-provided-questions",
    "href": "mp02.html#nostalgia-and-remakes-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Nostalgia and Remakes: Instructor-Provided Questions",
    "text": "Nostalgia and Remakes: Instructor-Provided Questions\nNow that you have found a target genre and key talent for your project, you need a story. Like any good development executive, your first instinct should be to produce a remake of a classic film in the genre."
  },
  {
    "objectID": "mp02.html#task-6-finding-a-classic-movie-to-remake-instructor-provided-questions",
    "href": "mp02.html#task-6-finding-a-classic-movie-to-remake-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 6: Finding a Classic Movie to Remake: Instructor-Provided Questions",
    "text": "Task 6: Finding a Classic Movie to Remake: Instructor-Provided Questions\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.2\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nThe question does not specify genre, but does specify title type. I believe in the versatility of the team chosen so let’s find the movie.\n\ntask6_table &lt;- NEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(titleType.x == \"movie\", numVotes &gt; 100000)\n\ntask6_table |&gt; \n  group_by(tconst, primaryTitle.x, genres.x) |&gt;\n  summarise( Average_Weighted_Rating = mean(weighted_rating), .groups = \"drop\") |&gt;\n  arrange(desc(Average_Weighted_Rating)) |&gt;\n  select(primaryTitle.x, genres.x, Average_Weighted_Rating) |&gt;\n  head(10) |&gt;\n  ungroup()\n\n# A tibble: 10 × 3\n   primaryTitle.x                                genres.x Average_Weighted_Rat…¹\n   &lt;chr&gt;                                         &lt;chr&gt;                     &lt;dbl&gt;\n 1 The Shawshank Redemption                      Drama                       9.3\n 2 The Godfather                                 Crime,D…                    9.2\n 3 12 Angry Men                                  Crime,D…                    9  \n 4 The Godfather Part II                         Crime,D…                    9  \n 5 Schindler's List                              Biograp…                    9  \n 6 The Lord of the Rings: The Return of the King Action,…                    9  \n 7 The Dark Knight                               Action,…                    9  \n 8 Pulp Fiction                                  Crime,D…                    8.9\n 9 The Lord of the Rings: The Fellowship of the… Action,…                    8.9\n10 The Good, the Bad and the Ugly                Adventu…                    8.8\n# ℹ abbreviated name: ¹​Average_Weighted_Rating\n\n\nI have created the table above filtering our data for movies with more than 100,000 votes. The top movie is The Shawshank Redemption and its genre is Drama which fits perfectly with our team. As we can see below, there is only one title with that name in our data set.\n\nTITLE_BASICS |&gt;\n  filter(primaryTitle == \"The Shawshank Redemption\") |&gt;\n  select(-tconst, -originalTitle, -isAdult, -endYear)\n\n  titleType             primaryTitle startYear runtimeMinutes genres\n1     movie The Shawshank Redemption      1994            142  Drama\n\n\nThe following people participated in this production in different capacities. They are less than 100 years old and they might be alive. We can contact them to secure rights.\n\nNEW_TITLE_RATINGS_WITH_BIRTHYEAR |&gt;\n  filter(tconst == \"tt0111161\") |&gt;\n  filter(is.na(deathYear)) |&gt;\n  filter(birthYear &gt; \"1924\") |&gt;\n  select(primaryName, primaryProfession)\n\n             primaryName                               primaryProfession\n1            Tim Robbins                         actor,producer,director\n2            Tim Robbins                         actor,producer,director\n3            Tim Robbins                         actor,producer,director\n4            Tim Robbins                         actor,producer,director\n5         Morgan Freeman                         actor,producer,director\n6         Morgan Freeman                         actor,producer,director\n7         Morgan Freeman                         actor,producer,director\n8         Morgan Freeman                         actor,producer,director\n9             Bob Gunton                actor,soundtrack,archive_footage\n10            Bob Gunton                actor,soundtrack,archive_footage\n11            Bob Gunton                actor,soundtrack,archive_footage\n12            Bob Gunton                actor,soundtrack,archive_footage\n13        William Sadler                         actor,producer,director\n14        William Sadler                         actor,producer,director\n15        William Sadler                         actor,producer,director\n16        William Sadler                         actor,producer,director\n17          Clancy Brown                       actor,producer,soundtrack\n18          Clancy Brown                       actor,producer,soundtrack\n19          Clancy Brown                       actor,producer,soundtrack\n20          Clancy Brown                       actor,producer,soundtrack\n21           Gil Bellows                         actor,producer,director\n22           Gil Bellows                         actor,producer,director\n23           Gil Bellows                         actor,producer,director\n24           Gil Bellows                         actor,producer,director\n25          Mark Rolston                   actor,sound_department,writer\n26          Mark Rolston                   actor,sound_department,writer\n27          Mark Rolston                   actor,sound_department,writer\n28          Mark Rolston                   actor,sound_department,writer\n29        Jeffrey DeMunn                           actor,archive_footage\n30        Jeffrey DeMunn                           actor,archive_footage\n31        Jeffrey DeMunn                           actor,archive_footage\n32        Jeffrey DeMunn                           actor,archive_footage\n33     Larry Brandenburg                             actor,archive_sound\n34     Larry Brandenburg                             actor,archive_sound\n35     Larry Brandenburg                             actor,archive_sound\n36     Larry Brandenburg                             actor,archive_sound\n37        Frank Darabont                        writer,producer,director\n38        Frank Darabont                        writer,producer,director\n39        Frank Darabont                        writer,producer,director\n40        Frank Darabont                        writer,producer,director\n41          Stephen King                           writer,producer,actor\n42          Stephen King                           writer,producer,actor\n43          Stephen King                           writer,producer,actor\n44          Stephen King                           writer,producer,actor\n45        Frank Darabont                        writer,producer,director\n46        Frank Darabont                        writer,producer,director\n47        Frank Darabont                        writer,producer,director\n48        Frank Darabont                        writer,producer,director\n49         Thomas Newman            music_department,composer,soundtrack\n50         Thomas Newman            music_department,composer,soundtrack\n51         Thomas Newman            music_department,composer,soundtrack\n52         Thomas Newman            music_department,composer,soundtrack\n53         Roger Deakins cinematographer,camera_department,miscellaneous\n54         Roger Deakins cinematographer,camera_department,miscellaneous\n55         Roger Deakins cinematographer,camera_department,miscellaneous\n56         Roger Deakins cinematographer,camera_department,miscellaneous\n57 Richard Francis-Bruce    editor,editorial_department,sound_department\n58 Richard Francis-Bruce    editor,editorial_department,sound_department\n59 Richard Francis-Bruce    editor,editorial_department,sound_department\n60 Richard Francis-Bruce    editor,editorial_department,sound_department"
  },
  {
    "objectID": "mp02.html#task-7-write-and-deliver-your-pitch-instructor-provided-questions",
    "href": "mp02.html#task-7-write-and-deliver-your-pitch-instructor-provided-questions",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Task 7: Write and Deliver Your Pitch: Instructor-Provided Questions",
    "text": "Task 7: Write and Deliver Your Pitch: Instructor-Provided Questions\nNow that you have completed your analysis, write an “elevator pitch” of approximately 200-250 words for your proposed Hollywood project. This is the pitch you will bring to the studio head (your boss); if the studio head likes your pitch, you will be given a small sum of money to start securing the story rights and locking down tentative deals with key talent.\nYour pitch needs to synthesize the analysis above into two to three quick and compelling points. (E.g., “The market for animated young adult horror musicals has grown 200% in the past decade” or “Over 90% of Director D’s movies are successes.”) You need to present the strongest argument for each element of your pitch, including genre, director, actors, and story.\nIf your boss approves the pitch, you will need to have a brief trailer ready for the next quarterly earnings call. The marketing department has asked that you prepare a classic 90’s style teaser for them. Adapt the following cliched formula for your pitch.\n\nFrom director D, the visionary mind between N1; and From actor A, beloved star of N2; and From actor A2, Hollywood icon of genre G, Comes the timeless tail N3 A story of TOPIC, TOPIC, and TOPIC Coming soon to a theater near you.\n\nIf you’re creatively-minded, you could have some fun here using Generative tools to draft a script or mock up a movie poster for your pitch.\n\nTask 7: Write and Deliver Your Pitch\nBased on IMDb data, I conducted research in order to find the best industry to launch our new project. After creating a metric of success that takes into consideration the public awareness of the title, I found that Drama is the best genre for our project. To work on this project, we will need the best talent. Therefore, I have selected the following individuals for the following roles (see known for titles next to their names):\n\nActress: Storm Reid - Stand Still Like the Hummingbird\nActor: Richard Schiff - The West Wing\nDirector: Daniel Attias - Kingdom Come\n\nOur team has the experience of our Director and our Actor, both of which have many years of experience in the industry. We also have a young Actress, who despite her youth, has been able to achieve great success by being part of many successful and well known titles. All of them, have been part of titles with an average weighted rating above 7.5. This measure reflects public awareness and quality of the production. We would like to tap into the public awareness and good reception of the movie The Shawshank Redemption and make a remake of this classic. This is a great movie in the Drama genre with an average weighted rating of 9.3.\nGiven the public’s strong demand for Drama titles, and the strong team I have selected, I am confident we can create a successful title.\n\nCredits to Michael Weylandt who provided a great part of the code for the data gathering and cleaning process."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "Warning: package 'tmap' was built under R version 4.4.2\n\n\n\n\nThe United States is a democratic country, and therefore elections are very important. Through casting their vote, US citizens have a say on who will be their president and other political leaders. However, the system used for selecting a president since the inception of this country, the Electoral College, allows for situations in which a presidential candidate might win the election without winning the popular vote. This is because each state gets as many votes as the number of congress Senators and Representatives. Each state has at least 2 Senators despite the its population size.\nSome politicians such as Sen. Elizabeth Warren (D-Mass) believe that the electoral college should be abolished. She explains that having a national vote would ensure that every vote matters equally towards selecting a president.\nHouse member Alexandria Ocasio-Cortez argues that there is a bias favoring rural voters.\n\n\n\nTo review the claims mentioned above, We will use data from multiple sources.\n\n\nWe will manually download data from the MIT Election Data Science Lab, which collects votes from all biennial congressional races in all 50 states here.1 The data is from 1976 to 2022.\nFurthermore, we will also manually download statewide presidential vote counts from 1976 to 2020 from here.2\n\n\nElection Votes Data\nhouse_votes &lt;- read.csv(\"1976-2022-house.csv\")\npresidential_votes &lt;- read.csv(\"1976-2020-president.csv\")\n\n\n\n\n\nWe would like to visualize the data gathered above. Therefore, we will use the following code to automatically download and load files that will help us to visualize the data into R.\nWe will use shapefiles for all US congressional districts from 1789 to 2012 provided by Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis here.\n\n\nCongressional Districts Data Download\ntd &lt;- tempdir()\nfor (i in 94:112) {\n  fname &lt;- paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n  \n  # Check if file exists, if not, download it\n  if (!file.exists(fname)) {\n    url &lt;- paste0(\"https://cdmaps.polisci.ucla.edu/shp/\", fname)\n    download.file(url, destfile = fname)\n  }\n  \n  # Unzip and read shapefile\n  zip_contents &lt;- unzip(fname, exdir = td)\n  shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  sf_data &lt;- read_sf(shp_file)\n  \n  # Assign data to variable\n  assign(paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n}\n\n\n\n\n\nWe will also download and load into r Congressional Boundary Files from the year 2014 to the present. The data is found under the FTP Archive here\n\n\nCongressional Boundary Data Download\ntd &lt;- tempdir()\n\n\nfor (i in 2014:2015) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd114.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\nfor (i in 2016:2017) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd115.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\nfor (i in 2018:2022) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd116.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\n\n\n\n\nWe will start to analyze the data by finding which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022. This will help us see the impact over time of voting power gained and lost by those states. This is because the number of electoral votes, which count towards presidential elections, is 2 (number of senators for each state), plus the number of House Representatives (indicated by the number of congressional districts).\n\n\nData Exploration\nhouse_seats_1976_2022 &lt;- house_votes |&gt;\n  filter(year %in% c(1976, 2022))\n\ndistrict_count &lt;- house_seats_1976_2022 |&gt;\n  group_by(year, state) |&gt;\n  distinct(district, .keep_all = TRUE) |&gt;\n  summarise(count = n()) |&gt;\n  ungroup() |&gt;\n  arrange(year, desc(count))\n\nnet_change &lt;- district_count |&gt;\n  pivot_wider(names_from = \"year\", values_from = \"count\") |&gt;\n  mutate(net_change = `2022` - `1976`)\n\ntop_3_gains &lt;- net_change |&gt;\n  arrange(desc(net_change)) |&gt;\n  head(3)\n\nbottom_3_losses &lt;- net_change |&gt;\n  arrange(net_change) |&gt;\n  head(3)\n\nnet_change &lt;- rbind(top_3_gains, bottom_3_losses)\n\n\nggplot(net_change, aes(x = reorder(state, net_change), y = net_change, fill = ifelse(net_change &gt; 0, \"Gain\", \"Loss\"))) + \n  geom_bar(stat = \"identity\") + \n  labs(title = \"Net Change in Number of House of Representatives Seats (1976-2022)\", \n       x = \"State\", y = \"Net Change\", fill = \"Change Type\") + \n  theme_classic() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\"green\", \"red\"))\n\n\n\n\n\n\n\n\n\nAs we seen in the graph above, Over the years 1976 to 2022, Texas has been the state that has gained the most House of Representative seats, and therefore has gained the most electoral votes. While New York has lost the most electoral votes.\nNow, as part of the data exploration, we will check whether there are seat elections that could have been influence by certain details on the ballot. For example, candidates in New York can be listed under many different political parties. Let’s check elections in 2020 to see if there are any candidates meeting this criteria.\n\n\nCongressional Candidate Table\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  group_by(year, candidate) |&gt;\n  summarise(count = n()) |&gt;\n  filter(count &gt; 1) |&gt;\n  arrange(desc(count)) |&gt;\n  head(1) |&gt;\n datatable()\n\n\n\n\n\n\nAs we see above, Andrew Garbarino was running as part of many political parties. The details are as follows:\n\n\nCongressional Candidate: Political Parties\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, candidate == \"ANDREW R GARBARINO\") |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  datatable()\n\n\n\n\n\n\nWe can see below that Mr. Garbarino has the majority of the individual candidate votes.\n\n\nCongressional Candidates: NY 2020 | District 2\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, district == \"2\", !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  arrange(desc(totalvotes)) |&gt;\n datatable()\n\n\n\n\n\n\nNow, let’s put all of the the votes together.\n\n\nCongressional Candidates: NY 2020 | District 2 | Fusion of votes\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, district == \"2\", !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  group_by(candidate) |&gt;\n  summarise(total_candidate_votes = sum(candidatevotes)) |&gt;\n  arrange(desc(total_candidate_votes)) |&gt;\n  datatable()\n\n\n\n\n\n\nWe saw that the margin of gain when looking only at candidate votes was very small. Therefore, it is very important we take into consideration the fact that some candidates might have votes under many different parties. In this case, the winner was the same in both scenarios, but we see the potential for a mistake if the votes are not combined.\nNow, lets compare vote count between presidential candidates and congressional candidates. There are two major political parties in the US. Therefore, we will look only at these two.\nFor the Democratic party, we see that starting from the year 1996, the total votes for the presidential candidate have been higher than the total votes for all congressional candidates of the same party.\n\n\nCongressional Candidates vs Congressional Candidates Democrat\n# Summarize votes\npresidential_votes_summarized &lt;- presidential_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party_simplified) |&gt; \n  rename(party = party_simplified)\n\nhouse_votes_summarized &lt;- house_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party)\n\n# Combine votes\ncombined_votes &lt;- bind_rows(presidential_votes_summarized, house_votes_summarized) |&gt; \n  filter(year %% 4 == 0) |&gt; \n  group_by(year, state, office, party) |&gt; \n  summarise(sum(candidatevotes)) |&gt; \n  ungroup()\n\n# Filter Democratic votes\ndemocratic_votes &lt;- combined_votes |&gt; \n  filter(party == \"DEMOCRAT\")\n\ndemocratic_presidential_votes &lt;- democratic_votes |&gt; \n  filter(office == \"US PRESIDENT\")\n\ndemocratic_house_votes &lt;- democratic_votes |&gt; \n  filter(office == \"US HOUSE\")\n\n# Calculate vote difference\nvote_difference &lt;- democratic_presidential_votes$`sum(candidatevotes)` - democratic_house_votes$`sum(candidatevotes)`\n\n# Prepare data for plotting\nplot_data &lt;- rbind(\n  democratic_presidential_votes %&gt;% mutate(office = \"President\"),\n  democratic_house_votes %&gt;% mutate(office = \"House\")\n)\n\n# Plot comparison\nggplot(data = plot_data, aes(x = year, y = `sum(candidatevotes)`, fill = office)) + \n  geom_col(position = position_dodge()) + \n  labs(title = \"Democratic Votes Comparison\") + \n  theme_classic() +\n  labs(y = \"Votes\", x = \"Year\")\n\n\n\n\n\n\n\n\n\nFor the Republican party, we see that the total votes for the presidential candidate have been higher than the total votes for all congressional candidates of the same party for all years except for three of them: 1992, 1996, and 2016.\n\n\nCongressional Candidates vs Congressional Candidates Republican\n# Summarize votes\npresidential_votes_summarized &lt;- presidential_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party_simplified) |&gt; \n  rename(party = party_simplified)\n\nhouse_votes_summarized &lt;- house_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party)\n\n# Combine votes\ncombined_votes &lt;- bind_rows(presidential_votes_summarized, house_votes_summarized) |&gt; \n  filter(year %% 4 == 0) |&gt; \n  group_by(year, state, office, party) |&gt; \n  summarise(sum(candidatevotes)) |&gt; \n  ungroup()\n\n# Filter Republican votes\nrepublican_votes &lt;- combined_votes |&gt; \n  filter(party == \"REPUBLICAN\")\n\nrepublican_presidential_votes &lt;- republican_votes |&gt; \n  filter(office == \"US PRESIDENT\")\n\nrepublican_house_votes &lt;- republican_votes |&gt; \n  filter(office == \"US HOUSE\")\n\n# Calculate vote difference\nvote_difference &lt;- republican_presidential_votes$`sum(candidatevotes)` - republican_house_votes$`sum(candidatevotes)`\n\n# Prepare data for plotting\nplot_data &lt;- rbind(\n  republican_presidential_votes %&gt;% mutate(office = \"President\"),\n  republican_house_votes %&gt;% mutate(office = \"House\")\n)\n\n# Plot comparison\nggplot(data = plot_data, aes(x = year, y = `sum(candidatevotes)`, fill = office)) + \n  geom_col(position = position_dodge()) + \n  labs(title = \"Republican Votes Comparison\") + \n  theme_classic() +\n  labs(y = \"Votes\", x = \"Year\")\n\n\n\n\n\n\n\n\n\n\n\n\nLet us visualize the 2000 Presidential Election Electoral College Results. First, I would like to share the map of the US. Here, we can clearly see each state filled in different colors. Also, within each state we can see the congressional districts. This is important, because the more congressional districts a state has, the more electoral votes it gets.\n\n\nMap of US States and Congressional Districts\n#106th congress dates are   January 6, 1999 to December 15, 2000\n\n\n#Plot with congressional divisions\nggplot(\n  districts106_sf,\n  aes(geometry = geometry, fill = STATENAME)) +\n  geom_sf() +\n  theme_classic() +\n  coord_sf(xlim = c(-200, -60)) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "The United States is a democratic country, and therefore elections are very important. Through casting their vote, US citizens have a say on who will be their president and other political leaders. However, the system used for selecting a president since the inception of this country, the Electoral College, allows for situations in which a presidential candidate might win the election without winning the popular vote. This is because each state gets as many votes as the number of congress Senators and Representatives. Each state has at least 2 Senators despite the its population size.\nSome politicians such as Sen. Elizabeth Warren (D-Mass) believe that the electoral college should be abolished. She explains that having a national vote would ensure that every vote matters equally towards selecting a president.\nHouse member Alexandria Ocasio-Cortez argues that there is a bias favoring rural voters."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "Mini-Project #03:",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCitation: MIT Election Data and Science Lab, 2017, “U.S. House 1976–2022”, https://doi.org/10.7910/DVN/IG0UN2, Harvard Dataverse, V13; 1976-2022-house.tab [fileName], UNF:6:Ky5FkettbvohjTSN/IVldA== [fileUNF]↩︎\nCitation: MIT Election Data and Science Lab, 2017, “U.S. President 1976–2020”, https://doi.org/10.7910/DVN/42MVDX, Harvard Dataverse, V8; 1976-2020-president.tab [fileName], UNF:6:a2yzwWNbv+Eff8aqVmkZKA== [fileUNF]↩︎"
  },
  {
    "objectID": "mp03.html#data",
    "href": "mp03.html#data",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "To review the claims mentioned above, We will use data from multiple sources.\n\n\nWe will manually download data from the MIT Election Data Science Lab, which collects votes from all biennial congressional races in all 50 states here.1 The data is from 1976 to 2022.\nFurthermore, we will also manually download statewide presidential vote counts from 1976 to 2020 from here.2\n\n\nElection Votes Data\nhouse_votes &lt;- read.csv(\"1976-2022-house.csv\")\npresidential_votes &lt;- read.csv(\"1976-2020-president.csv\")\n\n\n\n\n\nWe would like to visualize the data gathered above. Therefore, we will use the following code to automatically download and load files that will help us to visualize the data into R.\nWe will use shapefiles for all US congressional districts from 1789 to 2012 provided by Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis here.\n\n\nCongressional Districts Data Download\ntd &lt;- tempdir()\nfor (i in 94:112) {\n  fname &lt;- paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n  \n  # Check if file exists, if not, download it\n  if (!file.exists(fname)) {\n    url &lt;- paste0(\"https://cdmaps.polisci.ucla.edu/shp/\", fname)\n    download.file(url, destfile = fname)\n  }\n  \n  # Unzip and read shapefile\n  zip_contents &lt;- unzip(fname, exdir = td)\n  shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  sf_data &lt;- read_sf(shp_file)\n  \n  # Assign data to variable\n  assign(paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n}\n\n\n\n\n\nWe will also download and load into r Congressional Boundary Files from the year 2014 to the present. The data is found under the FTP Archive here\n\n\nCongressional Boundary Data Download\ntd &lt;- tempdir()\n\n\nfor (i in 2014:2015) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd114.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\nfor (i in 2016:2017) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd115.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\nfor (i in 2018:2022) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd116.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}"
  },
  {
    "objectID": "mp03.html#1-introduction",
    "href": "mp03.html#1-introduction",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "The United States is a democratic country. Therefore, elections are very important. Through casting their vote, US citizens have a say on who will be their president and other political leaders. However, the system used for selecting a president since the inception of this country, the Electoral College, allows for situations in which a presidential candidate might win the election without winning the electoral vote.\nSome politicians such as Sen. Elizabeth Warren (D-Mass) believe that the electoral college should be abolished. She explains that having a national vote would ensure that every vote matters equally towards selecting a president.\nHouse member Alexandria Ocasio-Cortez argues that there is a bias favoring rural voters"
  },
  {
    "objectID": "mp03.html#2-data",
    "href": "mp03.html#2-data",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "To review the claims mentioned above, We will use data from multiple sources.\n\n\nWe will manually download data from the MIT Election Data Science Lab, which collects votes from all biennial congressional races in all 50 states here.1 The data is from 1976 to 2022.\nFurthermore, we will also manually download statewide presidential vote counts from 1976 to 2020 from here.2\n\n\nElection Votes Data\nhouse_votes &lt;- read.csv(\"1976-2022-house.csv\")\npresidential_votes &lt;- read.csv(\"1976-2020-president.csv\")\n\n\n\n\n\nWe would like to visualize the data gathered above. Therefore, we will use the following code to automatically download and load files that will help us to visualize the data into R.\nWe will use shapefiles for all US congressional districts from 1789 to 2012 provided by Jeffrey B. Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth C. Martis here.\n\n\nCongressional Districts Data Download\ntd &lt;- tempdir()\n\nfor (i in 94:112) {\n  fname &lt;- paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n  \n  if (!file.exists(fname)) {\n    url &lt;- paste0(\"https://cdmaps.polisci.ucla.edu/shp/\", fname)\n  \n    download.file(url, destfile = fname)\n    \n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\n\n\n\nWe will also download and load into r Congressional Boundary Files from the year 2014 to the present. The data is found under the FTP Archive here\n\n\nCongressional Boundary Data Download\ntd &lt;- tempdir()\n\n\nfor (i in 2014:2015) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd114.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\nfor (i in 2016:2017) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd115.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\nfor (i in 2018:2022) {\n  fname &lt;- paste0(\"tl_\", i, \"_us_cd116.zip\")\n  url &lt;- paste0(\"https://www2.census.gov/geo/tiger/TIGER\", i, \"/CD/\", fname)\n\n  if (!file.exists(fname)) {\n    download.file(url, destfile = fname)\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n    \n    assign(paste0(\"districts\", formatC(i, width = 2, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\n##Initial Exploration of Vote Count Data\nWe will start to analyze the data by finding which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022. This will help us see the impact over time of voting power gained and lost by those states. This is because the number of electoral votes, which count towards presidential elections, is 2 (number of senators for each state), plus the number of House Representatives (indicated by the number of congressional districts).\n\n\nData Exploration\nhouse_seats_1976_2022 &lt;- house_votes |&gt;\n  filter(year %in% c(1976, 2022))\n\ndistrict_count &lt;- house_seats_1976_2022 |&gt;\n  group_by(year, state) |&gt;\n  distinct(district, .keep_all = TRUE) |&gt;\n  summarise(count = n()) |&gt;\n  ungroup() |&gt;\n  arrange(year, desc(count))\n\nnet_change &lt;- district_count |&gt;\n  pivot_wider(names_from = \"year\", values_from = \"count\") |&gt;\n  mutate(net_change = `2022` - `1976`)\n\ntop_3_gains &lt;- net_change |&gt;\n  arrange(desc(net_change)) |&gt;\n  head(3)\n\nbottom_3_losses &lt;- net_change |&gt;\n  arrange(net_change) |&gt;\n  head(3)\n\nnet_change &lt;- rbind(top_3_gains, bottom_3_losses)\n\n\nggplot(net_change, aes(x = reorder(state, net_change), y = net_change, fill = ifelse(net_change &gt; 0, \"Gain\", \"Loss\"))) + \n  geom_bar(stat = \"identity\") + \n  labs(title = \"Net Change in Number of House of Representatives Seats (1976-2022)\", \n       x = \"State\", y = \"Net Change\", fill = \"Change Type\") + \n  theme_classic() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\"green\", \"red\"))\n\n\n\n\n\n\n\n\n\nAs we seen in the graph above, Over the years 1976 to 2022, Texas has been the state that has gained the most House of Representative seats, and therefore has gained the most electoral votes. While New York has lost the most electoral votes.\nNow, as part of the data exploration, we will check whether there are seat elections that could have been influence by certain details on the ballot. For example, candidates in New York can be listed under many different political parties. Let’s check elections in 2020 to see if there are any candidates meeting this criteria.\n\n\nCongressional Candidate Table\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  group_by(year, candidate) |&gt;\n  summarise(count = n()) |&gt;\n  filter(count &gt; 1) |&gt;\n  arrange(desc(count)) |&gt;\n  head(1) |&gt;\n datatable()\n\n\n\n\n\n\nAs we see above, Andrew Garbarino was running as part of many political parties. The details are as follows:\n\n\nCongressional Candidate: Political Parties\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, candidate == \"ANDREW R GARBARINO\") |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  datatable()\n\n\n\n\n\n\nWe can see below that Mr. Garbarino has the majority of the total votes, and also the majority of the individual candidate votes. However, the margin of gain when looking only at candidate votes is very small. Therefore, it is very important we take into consideration the fact that some candidates might have votes under many different parties.\n\n\nCongressional Candidates: NY 2020 | District 2\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, district == \"2\", !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  arrange(desc(totalvotes)) |&gt;\n datatable()\n\n\n\n\n\n\nNow, lets compare vote count between presidential candidates and congressional candidates."
  },
  {
    "objectID": "mp03.html#initial-exploration-of-vote-count-data",
    "href": "mp03.html#initial-exploration-of-vote-count-data",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "We will start to analyze the data by finding which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022. This will help us see the impact over time of voting power gained and lost by those states. This is because the number of electoral votes, which count towards presidential elections, is 2 (number of senators for each state), plus the number of House Representatives (indicated by the number of congressional districts).\n\n\nData Exploration\nhouse_seats_1976_2022 &lt;- house_votes |&gt;\n  filter(year %in% c(1976, 2022))\n\ndistrict_count &lt;- house_seats_1976_2022 |&gt;\n  group_by(year, state) |&gt;\n  distinct(district, .keep_all = TRUE) |&gt;\n  summarise(count = n()) |&gt;\n  ungroup() |&gt;\n  arrange(year, desc(count))\n\nnet_change &lt;- district_count |&gt;\n  pivot_wider(names_from = \"year\", values_from = \"count\") |&gt;\n  mutate(net_change = `2022` - `1976`)\n\ntop_3_gains &lt;- net_change |&gt;\n  arrange(desc(net_change)) |&gt;\n  head(3)\n\nbottom_3_losses &lt;- net_change |&gt;\n  arrange(net_change) |&gt;\n  head(3)\n\nnet_change &lt;- rbind(top_3_gains, bottom_3_losses)\n\n\nggplot(net_change, aes(x = reorder(state, net_change), y = net_change, fill = ifelse(net_change &gt; 0, \"Gain\", \"Loss\"))) + \n  geom_bar(stat = \"identity\") + \n  labs(title = \"Net Change in Number of House of Representatives Seats (1976-2022)\", \n       x = \"State\", y = \"Net Change\", fill = \"Change Type\") + \n  theme_classic() + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + \n  scale_fill_manual(values = c(\"green\", \"red\"))\n\n\n\n\n\n\n\n\n\nAs we seen in the graph above, Over the years 1976 to 2022, Texas has been the state that has gained the most House of Representative seats, and therefore has gained the most electoral votes. While New York has lost the most electoral votes.\nNow, as part of the data exploration, we will check whether there are seat elections that could have been influence by certain details on the ballot. For example, candidates in New York can be listed under many different political parties. Let’s check elections in 2020 to see if there are any candidates meeting this criteria.\n\n\nCongressional Candidate Table\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  group_by(year, candidate) |&gt;\n  summarise(count = n()) |&gt;\n  filter(count &gt; 1) |&gt;\n  arrange(desc(count)) |&gt;\n  head(1) |&gt;\n datatable()\n\n\n\n\n\n\nAs we see above, Andrew Garbarino was running as part of many political parties. The details are as follows:\n\n\nCongressional Candidate: Political Parties\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, candidate == \"ANDREW R GARBARINO\") |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  datatable()\n\n\n\n\n\n\nWe can see below that Mr. Garbarino has the majority of the individual candidate votes.\n\n\nCongressional Candidates: NY 2020 | District 2\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, district == \"2\", !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  arrange(desc(totalvotes)) |&gt;\n datatable()\n\n\n\n\n\n\nNow, let’s put all of the the votes together.\n\n\nCongressional Candidates: NY 2020 | District 2 | Fusion of votes\nhouse_votes |&gt;\n  filter(state == \"NEW YORK\", year == 2020, district == \"2\", !candidate %in% c(\"WRITEIN\", \"VOID\", \"UNDERVOTES\")) |&gt;\n  select(-state, -state_cen,-state_fips,-state_ic,-runoff,-special,-writein,-mode,-unofficial,-version,-fusion_ticket) |&gt;\n  group_by(candidate) |&gt;\n  summarise(total_candidate_votes = sum(candidatevotes)) |&gt;\n  arrange(desc(total_candidate_votes)) |&gt;\n  datatable()\n\n\n\n\n\n\nWe saw that the margin of gain when looking only at candidate votes was very small. Therefore, it is very important we take into consideration the fact that some candidates might have votes under many different parties. In this case, the winner was the same in both scenarios, but we see the potential for a mistake if the votes are not combined.\nNow, lets compare vote count between presidential candidates and congressional candidates. There are two major political parties in the US. Therefore, we will look only at these two.\nFor the Democratic party, we see that starting from the year 1996, the total votes for the presidential candidate have been higher than the total votes for all congressional candidates of the same party.\n\n\nCongressional Candidates vs Congressional Candidates Democrat\n# Summarize votes\npresidential_votes_summarized &lt;- presidential_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party_simplified) |&gt; \n  rename(party = party_simplified)\n\nhouse_votes_summarized &lt;- house_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party)\n\n# Combine votes\ncombined_votes &lt;- bind_rows(presidential_votes_summarized, house_votes_summarized) |&gt; \n  filter(year %% 4 == 0) |&gt; \n  group_by(year, state, office, party) |&gt; \n  summarise(sum(candidatevotes)) |&gt; \n  ungroup()\n\n# Filter Democratic votes\ndemocratic_votes &lt;- combined_votes |&gt; \n  filter(party == \"DEMOCRAT\")\n\ndemocratic_presidential_votes &lt;- democratic_votes |&gt; \n  filter(office == \"US PRESIDENT\")\n\ndemocratic_house_votes &lt;- democratic_votes |&gt; \n  filter(office == \"US HOUSE\")\n\n# Calculate vote difference\nvote_difference &lt;- democratic_presidential_votes$`sum(candidatevotes)` - democratic_house_votes$`sum(candidatevotes)`\n\n# Prepare data for plotting\nplot_data &lt;- rbind(\n  democratic_presidential_votes %&gt;% mutate(office = \"President\"),\n  democratic_house_votes %&gt;% mutate(office = \"House\")\n)\n\n# Plot comparison\nggplot(data = plot_data, aes(x = year, y = `sum(candidatevotes)`, fill = office)) + \n  geom_col(position = position_dodge()) + \n  labs(title = \"Democratic Votes Comparison\") + \n  theme_classic() +\n  labs(y = \"Votes\", x = \"Year\")\n\n\n\n\n\n\n\n\n\nFor the Republican party, we see that the total votes for the presidential candidate have been higher than the total votes for all congressional candidates of the same party for all years except for three of them: 1992, 1996, and 2016.\n\n\nCongressional Candidates vs Congressional Candidates Republican\n# Summarize votes\npresidential_votes_summarized &lt;- presidential_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party_simplified) |&gt; \n  rename(party = party_simplified)\n\nhouse_votes_summarized &lt;- house_votes |&gt; \n  select(year, state, state_po, state_cen, office, candidate, candidatevotes, totalvotes, party)\n\n# Combine votes\ncombined_votes &lt;- bind_rows(presidential_votes_summarized, house_votes_summarized) |&gt; \n  filter(year %% 4 == 0) |&gt; \n  group_by(year, state, office, party) |&gt; \n  summarise(sum(candidatevotes)) |&gt; \n  ungroup()\n\n# Filter Republican votes\nrepublican_votes &lt;- combined_votes |&gt; \n  filter(party == \"REPUBLICAN\")\n\nrepublican_presidential_votes &lt;- republican_votes |&gt; \n  filter(office == \"US PRESIDENT\")\n\nrepublican_house_votes &lt;- republican_votes |&gt; \n  filter(office == \"US HOUSE\")\n\n# Calculate vote difference\nvote_difference &lt;- republican_presidential_votes$`sum(candidatevotes)` - republican_house_votes$`sum(candidatevotes)`\n\n# Prepare data for plotting\nplot_data &lt;- rbind(\n  republican_presidential_votes %&gt;% mutate(office = \"President\"),\n  republican_house_votes %&gt;% mutate(office = \"House\")\n)\n\n# Plot comparison\nggplot(data = plot_data, aes(x = year, y = `sum(candidatevotes)`, fill = office)) + \n  geom_col(position = position_dodge()) + \n  labs(title = \"Republican Votes Comparison\") + \n  theme_classic() +\n  labs(y = \"Votes\", x = \"Year\")"
  },
  {
    "objectID": "mp03.html#visualization-of-the-2000-presidential-election-electoral-college-results",
    "href": "mp03.html#visualization-of-the-2000-presidential-election-electoral-college-results",
    "title": "Mini-Project #03:",
    "section": "",
    "text": "Let us visualize the 2000 Presidential Election Electoral College Results. First, I would like to share the map of the US. Here, we can clearly see each state filled in different colors. Also, within each state we can see the congressional districts. This is important, because the more congressional districts a state has, the more electoral votes it gets.\n\n\nMap of US States and Congressional Districts\n#106th congress dates are   January 6, 1999 to December 15, 2000\n\n\n#Plot with congressional divisions\nggplot(\n  districts106_sf,\n  aes(geometry = geometry, fill = STATENAME)) +\n  geom_sf() +\n  theme_classic() +\n  coord_sf(xlim = c(-200, -60)) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "New faculty hired at CUNY have 30 days to choose one of two retirement plans. In this project, I will be using R to make this important financial decision of choosing one of the plans by gathering and analyzing market and economic data.\nThese are some of the expected task that I will perform during this project:\n\nUse a password-protected API to acquire financial data\nUse resampling inference to estimate complex probability distributions\nSee how the optimal financial decision varies as a function of market returns\nInvestigate how demographic and actuarial assumptions, as well as individual risk-tolerances, change the optimal decision."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "New faculty hired at CUNY have 30 days to choose one of two retirement plans. In this project, I will be using R to make this important financial decision of choosing one of the plans by gathering and analyzing market and economic data.\nThese are some of the expected task that I will perform during this project:\n\nUse a password-protected API to acquire financial data\nUse resampling inference to estimate complex probability distributions\nSee how the optimal financial decision varies as a function of market returns\nInvestigate how demographic and actuarial assumptions, as well as individual risk-tolerances, change the optimal decision."
  },
  {
    "objectID": "mp04.html#background",
    "href": "mp04.html#background",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Background",
    "text": "Background\nPlease Note: Nothing in this document constitutes an official NYS, NYC, or CUNY statement about the retirement plans. This document omits several subtleties in the interest of pedagogical simplicity. If you are a potential or new CUNY employee who has stumbled across this document, please speak to your personal financial advisor, your Human Resources department, or your union representative for up-to-date and accurate retirement benefit information.\n\nCUNY Retirement Plans\nCUNY offers two retirement plans, the traditional defined-benefit Teachers Retirement System (TRS) plan and the newer defined-contribution Optional Retirement Plan (ORP).1\nFor this project, we may ignore the effect of taxes as both plans offer pre-tax retirement savings, so whichever plan has the greater (nominal, pre-tax) income will also yield the greater (post-tax) take-home amount.\n\nTeachers Retirement System\nThe TRS plan is a traditional pension plan: after retirement, the employer (CUNY) continues to pay employees a fraction of their salary until death. This type of plan is called a “defined-benefit” because the retirement pay (the benefit) is fixed a priori and the employer takes the market risk. If the market underperforms expectations, CUNY has to “pony up” and make up the gap; if the market overperforms expectations, CUNY pockets the excess balance. For further details, please see this link\n\n\nOptional Retirement Plan\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. Those investments grow “tax-free” until the employee begins to withdraw them upon retirement. If the employee does not use up the funds, they are passed down to that employee’s spouse, children, or other heirs; if the employee uses the funds too quickly and zeros out the account balance, no additional retirement funds are available. Though the employee hopefully still has Social Security retirement benefits and other savings to cover living expenses. This type of plan is called a defined-contribution plan as only the contributions to the retirement account are fixed by contract: the final balance depends on market factors outside of the employee’s control. For further details, please see this link\n\n\n\nData Sources - AlphaVantage & FRED\nIn this project, I will use two economic and financial data sources:\n\nAlphaVantage: a commercial stock market data provider\nFRED: the Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis\n\nTo start. I will load packages that I expect to use.\n\n\nSee the code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(readr)\nlibrary(sf)\nlibrary(gt)\nlibrary(knitr)\nlibrary(gganimate)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(lubridate)\n\n\nNow. I will create a function to get data from each data source through their API. Note, that I am using my own personal keys for each data source, and reading them from a local file for security. You can also create your own keys.\n\nCreate your FRED free API key here.\nCreate your AlphaVantage free API key here.\n\n\n\nSee the code\n#Importing API keys. I am using files saved on my computer for security given the keys are personal\n\nfredr_key &lt;- readLines(\"fredr_key\")\nalphavantage_key &lt;- readLines(\"alphavantage\")\n\n\n#Creating function to get data from FRED\n\nget_fred &lt;- function(id){\n  base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations?series_id=\"\n  res &lt;- GET(paste0(base_url,id,\"&api_key=\",fredr_key,\"&file_type=json\"))\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  json &lt;- fromJSON(res_content)\n  data &lt;-json$observations\n  print(res)\n  data &lt;- data %&gt;% mutate(value = as.numeric(value), #Convert to usable format\n                          date = as.Date(date))\n  return(data)\n}\n \n\n#Creating function to get data from AlphaVantage\n\nget_AV &lt;- function(symbol) {\n  base_url &lt;- \"https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY_ADJUSTED&symbol=\"\n  res &lt;- GET(paste0(base_url, symbol, \"&apikey=\", alphavantage_key, \"&outputsize=full&datatype=json\"))\n  res_content &lt;- content(res, as = \"text\", encoding = \"UTF-8\")\n  json &lt;- fromJSON(res_content)\n  data &lt;- json$`Monthly Adjusted Time Series`\n  data &lt;- bind_rows(data, .id = \"timestamp\")\n  data$timestamp &lt;- as.Date(data$timestamp)\n  return(data)\n}"
  },
  {
    "objectID": "mp04.html#set-up-and-exploration",
    "href": "mp04.html#set-up-and-exploration",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Set-Up and Exploration",
    "text": "Set-Up and Exploration\n\nData Acquisition\nTo begin the Monte Carlo analysis, I will need historical data covering the following:\n\nWage growth\nInflation\nUS Equity Market total returns\nInternational Equity Market total returns\nBond market total returns\nShort-term debt returns\n\n\n\nSee the code\navg_hourly_earnings &lt;- get_fred(\"CES0500000003\") #I will use this to estimate wage growth\n\n\nResponse [https://api.stlouisfed.org/fred/series/observations?series_id=CES0500000003&api_key=d2d08279a99ea8e400d4ac0135f723e3&file_type=json]\n  Date: 2024-12-05 02:51\n  Status: 200\n  Content-Type: application/json; charset=UTF-8\n  Size: 21.8 kB\n\n\nSee the code\nCORE_CPI &lt;- get_fred(\"CPILFESL\") #Core CPI excludes food and energy which are volatile components\n\n\nResponse [https://api.stlouisfed.org/fred/series/observations?series_id=CPILFESL&api_key=d2d08279a99ea8e400d4ac0135f723e3&file_type=json]\n  Date: 2024-12-05 02:51\n  Status: 200\n  Content-Type: application/json; charset=UTF-8\n  Size: 78.5 kB\n\n\nSee the code\nsp500_data &lt;- get_AV(\"SPY\") #Will be used to estimate US equity market total returns. We are using SPY which is an ETF that tracks the S&P500\n\nmsci_eafe_data &lt;- get_AV(\"EFA\") #Will be used to estimate International Equity Market total returns. ISHARES MSCI EAFE ETF \n\nIGIB_data &lt;- get_AV(\"IGIB\") #This is the ISHARES 5-10 YEAR INVESTMENT GRADE CORPORATE BOND ETF. We will use it as proxy for bond market returns\n\ntwo_year_treasury_yield_monthly &lt;- get_fred(\"DGS2\") #Will be used to estimate short-term debt returns\n\n\nResponse [https://api.stlouisfed.org/fred/series/observations?series_id=DGS2&api_key=d2d08279a99ea8e400d4ac0135f723e3&file_type=json]\n  Date: 2024-12-05 02:51\n  Status: 200\n  Content-Type: application/json; charset=UTF-8\n  Size: 1.21 MB\n\n\nAfter obtaining this data, I will clean it by renaming, creating, and updating some columns.\n\n\nSee the code\nwage_growth &lt;- avg_hourly_earnings |&gt; \n  mutate(avg_hourly_earnings = value) |&gt; \n  select(date, avg_hourly_earnings) |&gt; \n  mutate(\n    wage_change = avg_hourly_earnings - lag(avg_hourly_earnings),\n    wage_change_rate = (wage_change / lag(avg_hourly_earnings)) * 100\n  ) |&gt; \n  drop_na()\n\n\n\ninflation_data &lt;- CORE_CPI |&gt;\n   select(date, value) |&gt;\n   mutate(index_val = value) |&gt;\n   mutate(inflation_rate = ((index_val - lag(index_val)) / lag(index_val))*100) |&gt; \n  drop_na()\n\n\nUS_equities_returns &lt;- sp500_data |&gt;\n  select(timestamp, `1. open`, `5. adjusted close`, `7. dividend amount`) |&gt; \n  rename(\n    date = timestamp,\n    open_price = `1. open`,\n    adjusted_close_price = `5. adjusted close`,\n    dividend_amount = `7. dividend amount`\n  ) |&gt; \n   mutate(\n    across(c(open_price, adjusted_close_price, dividend_amount), as.numeric)\n  ) |&gt; \n  mutate(\n    daily_return = (adjusted_close_price - lag(adjusted_close_price)) / lag(adjusted_close_price)\n  ) |&gt; \n  drop_na()\n\n\n\ninternational_mkt_returns &lt;- msci_eafe_data |&gt; \n  select(timestamp, `1. open`, `5. adjusted close`, `7. dividend amount`) |&gt; \n  rename(\n    date = timestamp,\n    open_price = `1. open`,\n    adjusted_close_price = `5. adjusted close`,\n    dividend_amount = `7. dividend amount`\n  ) |&gt; \n  mutate(\n    across(c(open_price, adjusted_close_price, dividend_amount), as.numeric)\n  ) |&gt; \n  mutate(\n    daily_return = (adjusted_close_price - lag(adjusted_close_price)) / lag(adjusted_close_price)\n  ) |&gt; \n  drop_na()\n\n\nbond_mkt_returns &lt;- IGIB_data |&gt; \n  select(timestamp, `1. open`, `5. adjusted close`, `7. dividend amount`) |&gt; \n  rename(\n    date = timestamp,\n    open_price = `1. open`,\n    adjusted_close_price = `5. adjusted close`,\n    dividend_amount = `7. dividend amount`\n  ) |&gt; \n  mutate(\n    across(c(open_price, adjusted_close_price, dividend_amount), as.numeric)\n  ) |&gt; \n  mutate(\n    daily_return = (adjusted_close_price - lag(adjusted_close_price)) / lag(adjusted_close_price)\n  ) |&gt; \n  drop_na()\n\n\n\nst_debt_returns &lt;- two_year_treasury_yield_monthly |&gt;\n   select(date, value) |&gt;\n   rename(two_year_treas_yield = value) |&gt;\n   drop_na()\n\n\n\n\nInvestigation and Visualization of Input Data\nI have prepared the data for analysis. Now, I can start to investigate and visualize the data.\nI will start by visualizing the data.\nFrom the graphs below, we can see the effect of the 2020 pandemic. More specifically, we see there was a significant increased volatility in the wage change rate. Furthermore, returns across all asset classes were also volatile during that time.\n\n\nSee the code\nggplot(wage_growth, aes(x = date, y = wage_change_rate)) + \n  geom_line() + \n  labs(title = \"Wage Change Rate Over Time\", x = \"Date\", y = \"Wage Change Rate (%)\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nSee the code\nggplot(inflation_data, aes(x = date, y = inflation_rate)) + \n  geom_line() + \n  labs(title = \"Inflation Rate Over Time\", x = \"Date\", y = \"Inflation Rate (%)\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nSee the code\nggplot(US_equities_returns, aes(x = date, y = daily_return)) + \n  geom_line() + \n  labs(title = \"Historical Daily Returns of S&P 500\", x = \"Date\", y = \"Daily Return\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nSee the code\nggplot(international_mkt_returns, aes(x = date, y = daily_return)) + \n  geom_line() + \n  labs(title = \"International Market Returns Over Time\", x = \"Date\", y = \"Daily Return\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nSee the code\nggplot(bond_mkt_returns, aes(x = date, y = daily_return)) + \n  geom_line() + \n  labs(title = \"Bond Market Returns Over Time\", x = \"Date\", y = \"Daily Return\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nSee the code\nggplot(st_debt_returns, aes(x = date, y = two_year_treas_yield)) + \n  geom_line() + \n  labs(title = \"2-Year Treasury Yield Over Time\", x = \"Date\", y = \"Yield (%)\") + \n  theme_classic()\n\n\n\n\n\n\n\n\n\nNow, lets see a table with a summary of key statistics for various economic and financial indicators. I included data such as wage growth, inflation, and returns on different investment assets such as US equities, international markets, bonds, and short-term debt. The statistics presented include the mean, median, and standard deviation for each indicator, offering insights into the central tendency and variability of these economic metrics. Note all of these are based on the monthly data previously downloaded. The table shows, for example, that wage growth rate averages 0.26% monthly. From this table, I see something very interesting: US equities, international markets, and bonds have a negative average monthly return. However, those return numbers are very close to zero, as we can see in the table and the graphs above. Furthermore, the standard deviation is larger than the mean for those asset classes, which implies that the return at any given month can be positive, negative, or zero.\n\n\nSee the code\n# Calculate summary statistics for each data frame\nwage_growth_summary &lt;- summarise(wage_growth, \n                                     Mean = mean(wage_change_rate, na.rm = TRUE), \n                                     Median = median(wage_change_rate, na.rm = TRUE), \n                                     StdDev = sd(wage_change_rate, na.rm = TRUE))\n\ninflation_data_summary &lt;- summarise(inflation_data, \n                                         Mean = mean(inflation_rate, na.rm = TRUE), \n                                         Median = median(inflation_rate, na.rm = TRUE), \n                                         StdDev = sd(inflation_rate, na.rm = TRUE))\n\nUS_equities_returns_summary &lt;- summarise(US_equities_returns, \n                                               Mean = mean(daily_return, na.rm = TRUE), \n                                               Median = median(daily_return, na.rm = TRUE), \n                                               StdDev = sd(daily_return, na.rm = TRUE))\n\ninternational_mkt_returns_summary &lt;- summarise(international_mkt_returns, \n                                                     Mean = mean(daily_return, na.rm = TRUE), \n                                                     Median = median(daily_return, na.rm = TRUE), \n                                                     StdDev = sd(daily_return, na.rm = TRUE))\n\nbond_mkt_returns_summary &lt;- summarise(bond_mkt_returns, \n                                            Mean = mean(daily_return, na.rm = TRUE), \n                                            Median = median(daily_return, na.rm = TRUE), \n                                            StdDev = sd(daily_return, na.rm = TRUE))\n\nst_debt_returns_summary &lt;- summarise(st_debt_returns, \n                                           Mean = mean(two_year_treas_yield, na.rm = TRUE), \n                                           Median = median(two_year_treas_yield, na.rm = TRUE), \n                                           StdDev = sd(two_year_treas_yield, na.rm = TRUE))\n\n# Combine the summary statistics into a single data frame\nsummary_df &lt;- rbind(\n  data.frame(Data_Frame = \"Wage Growth\", wage_growth_summary),\n  data.frame(Data_Frame = \"Inflation Data\", inflation_data_summary),\n  data.frame(Data_Frame = \"US Equities Returns\", US_equities_returns_summary),\n  data.frame(Data_Frame = \"International Market Returns\", international_mkt_returns_summary),\n  data.frame(Data_Frame = \"Bond Market Returns\", bond_mkt_returns_summary),\n  data.frame(Data_Frame = \"Short-Term Debt Returns\", st_debt_returns_summary)\n)\n\n# Create a pretty table with the summary statistics\ngt(summary_df) |&gt; \n  tab_header(title = \"Summary Statistics\")\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\nData_Frame\nMean\nMedian\nStdDev\n\n\n\n\nWage Growth\n0.256590989\n0.241911098\n0.34426954\n\n\nInflation Data\n0.298851156\n0.256471716\n0.24548277\n\n\nUS Equities Returns\n-0.005221692\n-0.012219147\n0.04482886\n\n\nInternational Market Returns\n-0.003635796\n-0.011133401\n0.04991070\n\n\nBond Market Returns\n-0.002848834\n-0.003891972\n0.01724085\n\n\nShort-Term Debt Returns\n4.991983669\n4.740000000\n3.71260511\n\n\n\n\n\n\n\nNow, I will use the data gathered to create a table to compare the expected income for the first month of retirement. Note I am making key assumptions:\n\nEmployee joined CUNY in the first month of the historical data\nEmployee retired from CUNY at the end of the final month of data\n\nFurther assumptions are detailed in the code.\n\n\nSee the code\n# Define assumptions\nsalary_starting &lt;- 150000 # Salary range: $145,000-$155,000 FOR Assistant Professor- Information Systems (Data Science) (https://cuny.jobs/new-york-ny/assistant-professor-information-systems-data-science/D8B28B053E1A42CDB9F3BD60064D3617/job/)\n\nworking_time_yr &lt;- as.numeric(difftime(max(wage_growth$date), min(wage_growth$date), units = \"days\")) / 365.25\nage_of_retirement &lt;- 65\nretirement_month &lt;- max(wage_growth$date)\n\n# Function to calculate TRS monthly pension\ntrs_calculation &lt;- function(starting_salary, wage_growth_data, inflation_data, years_of_service) {\n  # Initialize salary and a vector to hold yearly salaries\n  current_salary &lt;- starting_salary\n  salary_history &lt;- numeric(years_of_service)\n  \n  # Loop through each year to calculate salary with growth and inflation\n  for (year in 1:years_of_service) {\n    # Get the corresponding wage growth and inflation rate for the current year\n    growth_rate &lt;- wage_growth_data$wage_change_rate[year %% nrow(wage_growth_data) + 1] / 100\n    inflation_rate &lt;- inflation_data$inflation_rate[year %% nrow(inflation_data) + 1] / 100\n    \n    # Update the salary for the current year\n    current_salary &lt;- current_salary * (1 + growth_rate - inflation_rate)\n    \n    # Store the updated salary\n    salary_history[year] &lt;- current_salary\n  }\n  \n  # Calculate Final Average Salary (FAS) using the last 3 years' salary\n  fas &lt;- mean(tail(salary_history, 3))\n  \n  # Calculate pension based on years worked\n  pension_factor &lt;- ifelse(years_of_service &lt;= 20, 0.0167, ifelse(years_of_service == 20, 0.0175, 0.35 + 0.02 * (years_of_service - 20)))\n  pension_amount &lt;- pension_factor * fas * years_of_service\n  \n  # Monthly pension is the annual pension divided by 12\n  monthly_pension &lt;- pension_amount / 12\n  \n  return(monthly_pension)\n}\n\n# Function to calculate ORP monthly income\norp_calculation &lt;- function(starting_salary, wage_growth_data, equity_data, bond_data, years_of_service, employer_contribution_rate = 0.08, withdrawal_rate = 0.04) {\n  # Initialize salary and account balance\n  current_salary &lt;- starting_salary\n  account_balance &lt;- 0\n  \n  # Loop through each year to calculate the salary, contributions, and account balance\n  for (year in 1:years_of_service) {\n    # Get wage growth, equity return, and bond return for the current year\n    growth_rate &lt;- wage_growth_data$wage_change_rate[year %% nrow(wage_growth_data) + 1] / 100\n    equity_return &lt;- equity_data$daily_return[year %% nrow(equity_data) + 1]\n    bond_return &lt;- bond_data$daily_return[year %% nrow(bond_data) + 1]\n    \n    # Calculate the weighted market return\n    market_return &lt;- 0.6 * equity_return + 0.4 * bond_return\n    \n    # Update the salary for the current year\n    current_salary &lt;- current_salary * (1 + growth_rate)\n    \n    # Calculate contributions\n    employee_contribution &lt;- current_salary * 0.06\n    employer_contribution &lt;- current_salary * employer_contribution_rate\n    total_contribution &lt;- employee_contribution + employer_contribution\n    \n    # Update the account balance with market returns and contributions\n    account_balance &lt;- account_balance * (1 + market_return) + total_contribution\n  }\n  \n  # Calculate the monthly withdrawal\n  monthly_withdrawal &lt;- account_balance * withdrawal_rate / 12\n  \n  return(monthly_withdrawal)\n}\n\n# Calculate TRS and ORP monthly income\ntrs_income &lt;- trs_calculation(\n  starting_salary = salary_starting,\n  wage_growth_data = wage_growth,\n  inflation_data = inflation_data,\n  years_of_service = working_time_yr\n)\n\norp_income &lt;- orp_calculation(\n  starting_salary = salary_starting,\n  wage_growth_data = wage_growth,\n  equity_data = US_equities_returns,\n  bond_data = bond_mkt_returns,\n  years_of_service = working_time_yr\n)\n\n# Create the income_table data frame\nincome_table &lt;- data.frame(\n  Plan = c(\"TRS\", \"ORP\"),\n  First_Month_Retirement_Income = c(round(trs_income, 2), round(orp_income, 2))\n)\n\n# Use gt to create the table\nlibrary(gt)\nincome_table %&gt;% \n  gt() %&gt;% \n  tab_header(\n    title = \"Income Comparison\",\n    subtitle = \"Comparison of First Month Retirement Income\"\n  ) %&gt;% \n  fmt_number(\n    columns = c(First_Month_Retirement_Income),\n    decimals = 2\n  ) %&gt;% cols_label\n\n\n\n\n\n\n\n\nIncome Comparison\n\n\nComparison of First Month Retirement Income\n\n\nPlan\nFirst_Month_Retirement_Income\n\n\n\n\nTRS\n3,895.37\n\n\nORP\n1,152.64\n\n\n\n\n\n\n\n\n\nLong-Term Average Analysis\nNow, let’s compare the retirement options TRS vs ORP. Based on our analysis, the TRS option provides a significantly higher average monthly income of $3,427.68, compared to the ORP option, which provides an average monthly income of $827.98. The maximum gap in monthly income between the two options is $3,326.89, and the minimum gap is $1,805.78. Additionally, our simulation suggests that there is a 0% probability of running out of funds before death for the ORP option based on the assumptions I made such as a starting salary of $50,000 (further assumptions can be seen in the code below). Overall, these results suggest that the TRS option may provide a better retirement source of income.This is because TSR guarantees income for life after retirement. However, we need to note that this is a long term analysis, and ORP’s returns rely heavily on compounded interest, so a small change in the assumed return will have a big effect on the monthly income of that plan.\n\n\nSee the code\nretirement_years &lt;- 25  # assuming 25 years of retirement\n\n\n\n# Simulate TRS Pension over retirement years\nsimulate_trs &lt;- function(trs_monthly_pension, retirement_years, inflation_data) {\n  pension_stream &lt;- numeric(retirement_years * 12)\n  for (month in 1:length(pension_stream)) {\n    if (month %% 12 == 1) {\n      inflation_rate &lt;- inflation_data$inflation_rate[(month %/% 12) %% nrow(inflation_data) + 1] / 100\n      inflation_rate &lt;- max(0.01, min(0.03, 0.5 * inflation_rate)) # Apply CPI adjustment rules\n    } else {\n      inflation_rate &lt;- 0\n    }\n    if (month == 1) {\n      pension_stream[month] &lt;- trs_monthly_pension\n    } else {\n      pension_stream[month] &lt;- pension_stream[month - 1] * (1 + inflation_rate)\n    }\n  }\n  return(pension_stream)\n}\n\n# Simulate ORP income stream with withdrawal rates and market returns\nsimulate_orp &lt;- function(orp_initial_balance, market_data, retirement_years, withdrawal_rate) {\n  withdrawal_stream &lt;- numeric(retirement_years * 12)\n  account_balance &lt;- orp_initial_balance\n  for (month in 1:length(withdrawal_stream)) {\n    if (account_balance &lt;= 0) {\n      withdrawal_stream[month] &lt;- 0\n    } else {\n      market_return &lt;- market_data$daily_return[(month - 1) %% nrow(market_data) + 1]\n      account_balance &lt;- account_balance * (1 + market_return)\n      withdrawal_amount &lt;- account_balance * withdrawal_rate / 12\n      withdrawal_stream[month] &lt;- min(account_balance, withdrawal_amount)\n      account_balance &lt;- account_balance - withdrawal_stream[month]\n    }\n  }\n  return(withdrawal_stream)\n}\n\n\n# Define the trs_calculation function\ntrs_calculation &lt;- function(starting_salary, wage_growth_data, inflation_data, years_of_service) {\n  # Calculate the final average salary (FAS)\n  fas &lt;- starting_salary * (1 + wage_growth_data$wage_growth_rate)^years_of_service\n  \n  # Calculate the TRS monthly pension\n  trs_monthly_pension &lt;- fas * 0.025  # Assuming 2.5% of FAS as the monthly pension\n  \n  return(trs_monthly_pension)\n}\n\n# Define the orp_calculation function\norp_calculation &lt;- function(starting_salary, wage_growth_data, equity_data, bond_data, years_of_service) {\n  # Calculate the total contributions to the ORP account\n  total_contributions &lt;- 0\n  for (i in 1:years_of_service) {\n    contribution_rate &lt;- get_contribution_rate(starting_salary * (1 + wage_growth_data$wage_growth_rate)^i)\n    employer_contribution_rate &lt;- get_employer_contribution_rate(i)\n    total_contributions &lt;- total_contributions + (starting_salary * (1 + wage_growth_data$wage_growth_rate)^i) * (contribution_rate + employer_contribution_rate)\n  }\n  \n  # Calculate the ORP initial account balance\n  orp_initial_balance &lt;- total_contributions\n  \n  return(orp_initial_balance)\n}\n\n\n#We also need to define the get_contribution_rate and get_employer_contribution_rate functions, which are used in the orp_calculation function:\n\n# Define the get_contribution_rate function\nget_contribution_rate &lt;- function(salary) {\n  if (salary &lt;= 45000) {\n    return(0.03)\n  } else if (salary &lt;= 55000) {\n    return(0.035)\n  } else if (salary &lt;= 75000) {\n    return(0.045)\n  } else if (salary &lt;= 100000) {\n    return(0.0575)\n  } else {\n    return(0.06)\n  }\n}\n\n# Define the get_employer_contribution_rate function\nget_employer_contribution_rate &lt;- function(years_of_service) {\n  if (years_of_service &lt;= 7) {\n    return(0.08)\n  } else {\n    return(0.10)\n  }\n}\n\n\n# Calculate TRS monthly pension\ntrs_monthly_pension &lt;- trs_calculation(\n  starting_salary = 50000,\n  wage_growth_data = data.frame(wage_growth_rate = 0.03),\n  inflation_data = data.frame(inflation_rate = 0.02),\n  years_of_service = 30\n)\n\n# Calculate ORP initial account balance\norp_initial_balance &lt;- orp_calculation(\n  starting_salary = 50000,\n  wage_growth_data = data.frame(wage_growth_rate = 0.03),\n  equity_data = data.frame(daily_return = 0.04),\n  bond_data = data.frame(daily_return = 0.02),\n  years_of_service = 30\n)\n\n\n\n# Simulate TRS income stream\ntrs_income_stream &lt;- simulate_trs(\n  trs_monthly_pension = trs_monthly_pension,\n  retirement_years = 25,\n  inflation_data = data.frame(inflation_rate = 0.02)\n)\n\n\n# Simulate ORP income stream\n\norp_income_stream &lt;- simulate_orp(\n  orp_initial_balance = orp_initial_balance,\n  market_data = data.frame(daily_return = 0.0005),\n  retirement_years = 25,\n  withdrawal_rate = 0.04\n)\n\n\n# Calculate average monthly income for TRS and ORP\ntrs_avg_income &lt;- mean(trs_income_stream)\norp_avg_income &lt;- mean(orp_income_stream)\n\n# Calculate maximum and minimum gap in monthly income between TRS and ORP\nincome_gap &lt;- trs_income_stream - orp_income_stream\nmax_income_gap &lt;- max(income_gap)\nmin_income_gap &lt;- min(income_gap)\n\n# Calculate probability of running out of funds before death for ORP\norp_runs_out_of_funds &lt;- sum(orp_income_stream == 0) / length(orp_income_stream)\n\n# Print the results\nprint(paste(\"Average monthly income for TRS: $\", round(trs_avg_income, 2)))\n\n\n[1] \"Average monthly income for TRS: $ 3427.68\"\n\n\nSee the code\nprint(paste(\"Average monthly income for ORP: $\", round(orp_avg_income, 2)))\n\n\n[1] \"Average monthly income for ORP: $ 827.98\"\n\n\nSee the code\nprint(paste(\"Maximum gap in monthly income between TRS and ORP: $\", round(max_income_gap, 2)))\n\n\n[1] \"Maximum gap in monthly income between TRS and ORP: $ 3326.89\"\n\n\nSee the code\nprint(paste(\"Minimum gap in monthly income between TRS and ORP: $\", round(min_income_gap, 2)))\n\n\n[1] \"Minimum gap in monthly income between TRS and ORP: $ 1805.78\"\n\n\nSee the code\nprint(paste(\"Probability of running out of funds before death for ORP: \", round(orp_runs_out_of_funds, 4)))\n\n\n[1] \"Probability of running out of funds before death for ORP:  0\"\n\n\n\n\nBootstrap (Monte Carlo) Comparison\nLet us run a Monte Carlo analysis for comparison of the retirement options.\n\n\nSee the code\n# Set the number of bootstrap samples\nn_boot &lt;- 200\n\n# Set the original data size\nn_samp &lt;- nrow(US_equities_returns)\n\n# Create a data frame with the original data\nUS_equities_returns |&gt;\n  # Use slice_sample to jointly resample pairs of data\n  slice_sample(n = n_samp * n_boot, replace = TRUE) |&gt;\n  # Create a resample ID for each bootstrap sample\n  mutate(resample_id = rep(1:n_boot, times = n_samp)) -&gt; bootstrap_samples\n\n# Define the withdrawal rate\nwithdrawal_rate &lt;- 0.04\n\n# Simulate TRS and ORP benefits for each bootstrap sample\ntrs_benefits &lt;- rep(NA, n_boot)\norp_benefits &lt;- rep(NA, n_boot)\nfor (i in 1:n_boot) {\n  trs_benefits[i] &lt;- simulate_trs(trs_monthly_pension, \n                                         retirement_years, \n                                         inflation_data)\n  orp_benefits[i] &lt;- simulate_orp(orp_initial_balance, \n                                        data.frame(daily_return = bootstrap_samples$daily_return[bootstrap_samples$resample_id == i]), \n                                        retirement_years, \n                                        withdrawal_rate)\n}\n\n\n###########\n###########\n#Analyze results\n###########\n###########\n\n\n# Calculate the average monthly income for each bootstrap sample\ntrs_avg_income &lt;- mean(trs_benefits)\norp_avg_income &lt;- mean(orp_benefits)\n\n# Calculate the probability that an ORP employee exhausts their savings before death\nprob_orp_exhausts &lt;- mean(orp_benefits == 0)\n\n# Calculate the probability that an ORP employee has a higher monthly income in retirement than a TRS employee\nprob_orp_higher &lt;- mean(orp_benefits &gt; trs_benefits)\n\n# Print the results\nprint(paste(\"Average monthly income for TRS: $\", round(trs_avg_income, 2)))\n\n\n[1] \"Average monthly income for TRS: $ 3034.08\"\n\n\nSee the code\nprint(paste(\"Average monthly income for ORP: $\", round(orp_avg_income, 2)))\n\n\n[1] \"Average monthly income for ORP: $ 1218.91\"\n\n\nSee the code\nprint(paste(\"Probability that an ORP employee exhausts their savings before death: \", round(prob_orp_exhausts, 4)))\n\n\n[1] \"Probability that an ORP employee exhausts their savings before death:  0\"\n\n\nSee the code\nprint(paste(\"Probability that an ORP employee has a higher monthly income in retirement than a TRS employee: \", round(prob_orp_higher, 4)))\n\n\n[1] \"Probability that an ORP employee has a higher monthly income in retirement than a TRS employee:  0\"\n\n\nNow that we have simulated the TRS and ORP benefits using the Monte Carlo strategy, let’s analyze the results. We calculated the average monthly income for each bootstrap sample and stored the results. We also calculated the probability that an ORP employee exhausts their savings before death and the probability that an ORP employee has a higher monthly income in retirement than a TRS employee.\nHere are the results:\n\nAverage monthly income for TRS: $3,034.08\nAverage monthly income for ORP: $1,221.44\nProbability that an ORP employee exhausts their savings before death: 0%\nProbability that an ORP employee has a higher monthly income in retirement than a TRS employee: 0%\n\nThese results suggest that, based on our simulations, the TRS plan provides a significantly higher average monthly income in retirement compared to the ORP plan. Additionally, our results indicate that there is no risk of an ORP employee exhausting their savings before death, and no chance of an ORP employee having a higher monthly income in retirement than a TRS employee.\nOverall, the TRS plan may be a more attractive option for someone seeking a secure guaranteed retirement income. However, as explained previously, small changes in assumptions can create big differences in retirement income. In this case, monthly returns of assets show a mean close to zero and a variance that implies potential positive, negative, and zero returns. Therefore, based on that data, we are seeing a lower retirement income for the ORP plan. However, with a small change in that assumption (growth rate of investments) the difference could be greater in either direction."
  },
  {
    "objectID": "mp04.html#data-driven-recommendation",
    "href": "mp04.html#data-driven-recommendation",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data-Driven Recommendation",
    "text": "Data-Driven Recommendation\nBased on the data obtained, processed, analyzed, it seems TRS provides a superior retirement plan compared to ORP. However, as previously explained, small changes in assumptions can create big differences in retirement income. In this case, monthly returns of assets show a mean close to zero and a variance that implies potential positive, negative, and zero returns. Therefore, based on that data, we are seeing a lower retirement income for the ORP plan. However, with a small change in that assumption (growth rate of investments) the difference could be greater in either direction. Thus, it is important to note that ultimately the decision might be influences by a number of factors such as personal preferences, personal expectations, and personal tax implications. For instance, a person expecting to have other sources of income at retirement, might be willing to select the ORP plan for the potential of higher returns based on the assumption that the market will perform well, and returns will compound greatly. On the other hand, someone relying only on this retirement plan for retirement income might be more inclined towards choosing the more secure and predictable plan: TRS."
  },
  {
    "objectID": "mp04.html#footnotes",
    "href": "mp04.html#footnotes",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCUNY employees have additional voluntary retirement savings plans, but these are non-exclusive and participation is voluntary, so we omit them from this analysis.↩︎"
  },
  {
    "objectID": "FINAL_PROJECT.html",
    "href": "FINAL_PROJECT.html",
    "title": "Final Presentation",
    "section": "",
    "text": "Loading Packages\nQ\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(DT)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(readr)\nlibrary(sf)\nlibrary(gt)\nlibrary(knitr)\nlibrary(gganimate)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(lubridate)\nlibrary(tidycensus)\nlibrary(fredr)\nfred_key &lt;- readLines(\"fredr_key\")\nfredr_set_key(fred_key)\ncensus_key &lt;- readLines(\"census_key.txt\", n = 1)\ncensus_api_key(census_key, install = TRUE, overwrite = TRUE)\n\n[1] \"955a70866abb78d3bd971d161b0a1739cd68a612\"\n\n\n\n#Unemployment Rate\n \nUNRATE &lt;- fredr(\n  series_id = \"UNRATE\",\n  observation_start = as.Date(\"1990-01-01\"),\n  observation_end = as.Date(\"2023-12-31\"),\n  frequency = \"m\"  # monthly\n  )\n\n\n# New York State Unemployment Rate (Seasonally Adjusted)\nny_unrate &lt;- fredr(\n  series_id = \"NYUR\",\n  observation_start = as.Date(\"2005-01-01\"),\n  observation_end = as.Date(\"2019-12-31\"),\n  frequency = \"a\"  # monthly\n)\n\n\n# Employment-Population Ratio: Measures percentage of population employed\n# Units:  Percent, Seasonally Adjusted\nEMRATIO &lt;- fredr(\n  series_id = \"EMRATIO\",\n  observation_start = as.Date(\"1990-01-01\"),\n  observation_end = as.Date(\"2023-12-31\"),\n  frequency = \"m\"  # monthly\n  )\n\n\n#|cache: TRUE\n\n# Population\n# Units: Thousands, Not Seasonally Adjusted\n\nPOPULATION &lt;- fredr(\n  series_id = \"POPTHM\",\n  observation_start = as.Date(\"1990-01-01\"),\n  observation_end = as.Date(\"2023-12-31\"),\n  frequency = \"m\"  # monthly\n  )\n\n\n# fredr_series_search_text(\n#      search_text = \"GENDER\",\n#      limit = 100L\n#  )\n\n\n# Find variables: https://api.census.gov/data/2020/dec/dhc/variables.html\n\n\n# population_data &lt;- get_acs_data(\n#   year = 2010:2023, \n#   geography = \"us\", variables = \"B01001_001\", \n#   survey = \"acs1\")\n\n\nunemployment_demographics &lt;- read.csv(\"unemployment_demographics.csv\")\nunemployment_demographics$Date &lt;- parse_date(unemployment_demographics$Date, format = \"%b-%Y\")\nunemployment_demographics &lt;- unemployment_demographics |&gt;\n  mutate(\n    Date = as.character(Date)\n  ) |&gt;\n  rename(date = Date) |&gt;\n  mutate(\n    across(-date, ~ as.numeric(gsub(\"%\", \"\", .), na.rm = TRUE))\n  )\n\nunemployment_demographics &lt;- unemployment_demographics |&gt;\n  mutate(date = as.Date(date, \"%Y-%m-%d\"))\n\nPrepare a table for the model\n\n#Creating a table for the regression model\n\nmodel_table &lt;- UNRATE |&gt;\n  select(date, unemployment_rate = value) |&gt;\n  left_join(POPULATION |&gt;\n              select(date, total_population = value), \n            by = \"date\") |&gt;\n  mutate(unemployed_population = (total_population * unemployment_rate / 100))\n\ndemographics &lt;- unemployment_demographics |&gt;\n  select(date, Women, Men, Black, Hispanic, White, X16.24, X25.54, X55.64, X65., Advanced.degree, \n         Bachelor.s.degree, High.school, Less.than.HS, Some.college) |&gt;\n  mutate(across(-date, as.numeric))\n\nmodel_table &lt;- model_table |&gt;\n  inner_join(demographics, join_by(date == date))\n\nDemographic Characteristics\n\n_001: Total population (as you’ve seen).\n_002: Male population.\n_003: Female population.\n\nRacial and Ethnic Groups\n\n_004: White alone.\n_005: Black or African American alone.\n_006: American Indian and Alaska Native alone.\n\nHispanic Origin\n\n_007: Hispanic or Latino.\n\nOther suffixes may indicate:\n\nAge groups (e.g., _010: Under 18).\nHousehold characteristics.\n\nResources\n\nAmerican Community Survey (ACS) documentation.\nACS variable index or documentation.\n\nConsult ACS documentation for specific variable meanings and suffixes.\n\n# Initialize data frame\nny_male_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  male_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B03002_002\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(male_population) &gt; 0) {\n    # Add year column\n    male_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_male_population_df &lt;- bind_rows(ny_male_population_df, male_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nny_female_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  female_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B03002_003\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(female_population) &gt; 0) {\n    # Add year column\n    female_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_female_population_df &lt;- bind_rows(ny_female_population_df, female_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\nglimpse(ny_female_population_df)\n\nRows: 15\nColumns: 6\n$ GEOID    &lt;chr&gt; \"36\", \"36\", \"36\", \"36\", \"36\", \"36\", \"36\", \"36\", \"36\", \"36\", \"…\n$ NAME     &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New York\", \"New York\", \"…\n$ variable &lt;chr&gt; \"B03002_003\", \"B03002_003\", \"B03002_003\", \"B03002_003\", \"B030…\n$ estimate &lt;dbl&gt; 11315746, 11628557, 11581226, 11642592, 11663600, 11289178, 1…\n$ moe      &lt;dbl&gt; 5380, 6496, 7055, 8870, 5905, 4490, 6416, 5742, 7594, 5763, 6…\n$ year     &lt;int&gt; 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2…\n\n\n\n# Initialize data frame\nny_white_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  white_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B02001_002\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(white_population) &gt; 0) {\n    # Add year column\n    white_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_white_population_df &lt;- bind_rows(ny_white_population_df, white_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nny_black_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  black_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B02001_002\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(black_population) &gt; 0) {\n    # Add year column\n    black_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_black_population_df &lt;- bind_rows(ny_black_population_df, black_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nny_hispanic_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  hispanic_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B03001_001\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(hispanic_population) &gt; 0) {\n    # Add year column\n    hispanic_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_hispanic_population_df &lt;- bind_rows(ny_hispanic_population_df, hispanic_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nny_black_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  black_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B02001_002\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(black_population) &gt; 0) {\n    # Add year column\n    black_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_black_population_df &lt;- bind_rows(ny_black_population_df, black_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nny_asian_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  asian_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B01001D_001\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(asian_population) &gt; 0) {\n    # Add year column\n    asian_population$year &lt;- year\n    \n    # Bind rows to data frame\n    ny_asian_population_df &lt;- bind_rows(ny_asian_population_df, asian_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nless_than_hs_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  less_than_hs_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B06009_002\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(less_than_hs_population) &gt; 0) {\n    # Add year column\n    less_than_hs_population$year &lt;- year\n    \n    # Bind rows to data frame\n    less_than_hs_population_df &lt;- bind_rows(less_than_hs_population_df, less_than_hs_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nhs_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  hs_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B06009_003\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(hs_population) &gt; 0) {\n    # Add year column\n    hs_population$year &lt;- year\n    \n    # Bind rows to data frame\n    hs_population_df &lt;- bind_rows(hs_population_df, hs_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n#some college or associates degree\n# Initialize data frame\nsome_college_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  some_college_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B06009_004\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(some_college_population) &gt; 0) {\n    # Add year column\n    some_college_population$year &lt;- year\n    \n    # Bind rows to data frame\n    some_college_population_df &lt;- bind_rows(some_college_population_df, some_college_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n# Initialize data frame\nbachelors_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  bachelors_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B06009_005\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(bachelors_population) &gt; 0) {\n    # Add year column\n    bachelors_population$year &lt;- year\n    \n    # Bind rows to data frame\n    bachelors_population_df &lt;- bind_rows(bachelors_population_df, bachelors_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n#Graduate or professional degree \n# Initialize data frame\n\ngraduate_population_df &lt;- data.frame()\n\n# Loop through years\nfor (year in 2005:2019) {\n  # Retrieve data\n  graduate_population &lt;- get_acs(\n    geography = \"state\",\n    state = \"NY\",\n    variables = \"B06009_005\",\n    year = year,\n    survey = \"acs1\"\n  )\n  \n  # Check for errors\n  if (nrow(graduate_population) &gt; 0) {\n    # Add year column\n    graduate_population$year &lt;- year\n    \n    # Bind rows to data frame\n    graduate_population_df &lt;- bind_rows(graduate_population_df, graduate_population)\n  } else {\n    print(paste(\"No data available for\", year))\n  }\n}\n\n\n#WoRK IN PROGRESS THEREFORE EVAL=FALSE\n\n#Creating a table for the regression model\n\nny_unrate$year &lt;- as.integer(format(ny_unrate$date, \"%Y\"))\n\n\nregression_model_table &lt;- ny_unrate |&gt;\n  left_join(ny_female_population_df |&gt;\n              select(female_pop = estimate, year), join_by(year == year)) |&gt;\n  select(-date)\n\n\n\nregression_model_table &lt;- ny_unrate |&gt;\n  select(year, unemployment_rate = value) |&gt;\n  left_join(ny_female_population_df |&gt;\n              select(female_pop = estimate, year), \n            by = \"year\") |&gt;\n  left_join(ny_male_population_df |&gt;\n              select(male_pop = estimate, year),\n            by = \"year\") |&gt;\n  left_join()"
  }
]